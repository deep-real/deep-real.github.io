<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Deep-REAL</title>
    <meta name="keywords" content="Deep-REAL, deep-real, udel, UD, university of delaware, Deep Robust &amp; Explainable AI Lab, computer vision, machine learning, xi peng, xipeng, Xi Peng, deepreal">
    <meta name="description" content="Deep Robust &amp; Explainable AI Lab. Deep-REAL is a research lab affiliated with the Department of Computer and Information Sciences and Data Science Institute, at the University of Delaware.">
    <link rel="icon" type="image/png" sizes="553x553" href="assets/img/favicon_circle.png">
    <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,700,700i,600,600i">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Actor&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Adamina&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alatsi&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Aldrich&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alegreya&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,400i,700,700i&amp;display=swap">
    <link rel="stylesheet" href="assets/css/Times%20New%20Roman%20Cyr.css">
    <link rel="stylesheet" href="assets/fonts/font-awesome.min.css">
    <link rel="stylesheet" href="assets/fonts/ionicons.min.css">
    <link rel="stylesheet" href="assets/css/back-to-top-button.css">
    <link rel="stylesheet" href="assets/css/Back-To-Top.css">
    <link rel="stylesheet" href="assets/css/back-to-top-scroll-1.css">
    <link rel="stylesheet" href="assets/css/back-to-top-scroll.css">
    <link rel="stylesheet" href="assets/css/Bold-BS4-Animated-Back-To-Top.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.5.2/animate.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/aos/2.3.4/aos.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/baguettebox.js/1.10.0/baguetteBox.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Swiper/6.4.8/swiper-bundle.min.css">
    <link rel="stylesheet" href="assets/css/MUSA_back-to-top-1.css">
    <link rel="stylesheet" href="assets/css/MUSA_back-to-top.css">
    <link rel="stylesheet" href="assets/css/Navigation-Clean.css">
    <link rel="stylesheet" href="assets/css/Popup-Element-Overlay-1.css">
    <link rel="stylesheet" href="assets/css/Popup-Element-Overlay.css">
    <link rel="stylesheet" href="assets/css/responsive-blog-card-slider.css">
    <link rel="stylesheet" href="assets/css/Simple-Slider.css">
    <link rel="stylesheet" href="assets/css/Swiper-Slider-Card-For-Blog-Or-Product-1.css">
    <link rel="stylesheet" href="assets/css/Swiper-Slider-Card-For-Blog-Or-Product.css">
    <link rel="stylesheet" href="assets/css/Tabbed_slider-1.css">
    <link rel="stylesheet" href="assets/css/Tabbed_slider.css">
    <link rel="stylesheet" href="assets/css/Ultimate-Testimonial-Slider.css">
    <link rel="stylesheet" href="assets/css/vanilla-zoom.min.css">
    <link rel="stylesheet" href="assets/css/WOWSlider-about-us-1.css">
    <link rel="stylesheet" href="assets/css/WOWSlider-about-us-2.css">
    <link rel="stylesheet" href="assets/css/WOWSlider-about-us.css">
</head>

<body>
    <nav class="navbar navbar-light navbar-expand-lg fixed-top bg-white clean-navbar" style="z-index: 10000;">
        <div class="container"><a class="navbar-brand logo" href="https://deep-real.github.io/"><img src="assets/img/logo.png" style="width: 120px;"></a><button data-bs-toggle="collapse" class="navbar-toggler" data-bs-target="#navcol-1"><span class="visually-hidden">Toggle navigation</span><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navcol-1">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link active" href="index.html">research</a></li>
                    <li class="nav-item"><a class="nav-link" href="people.html">PEOPLE</a></li>
                    <li class="nav-item"><a class="nav-link" href="publication.html">PUBLICATION</a></li>
                    <li class="nav-item"><a class="nav-link" href="resouce.html">RESOURCE</a></li>
                    <li class="nav-item"><a class="nav-link" href="opening.html">OPENING</a></li>
                </ul>
            </div>
        </div>
    </nav>
    <main class="page landing-page">
        <section class="clean-block clean-hero" style="color: rgba(33,37,41,0.27);background: url(&quot;assets/img/background.jpeg&quot;) no-repeat;background-size: cover;min-height: 700px;">
            <div class="text" style="max-width: 1000px;background: rgba(33,37,41,0.25);padding-top: 40px;padding-right: 40px;padding-bottom: 40px;padding-left: 40px;">
                <h2 style="text-align: center;font-family: Aldrich, sans-serif;font-size: calc(12pt + 1vw);margin-top: 15px;">Welcome to Deep Robust &amp; Explainable AI Lab!</h2>
                <p style="text-align: left;font-size: calc(10pt + 0.3vw);font-family: Montserrat, sans-serif;margin-bottom: 0px;">Deep-REAL is a research lab affiliated with the <span><a href="https://www.cis.udel.edu/" style="color: var(--bs-white);" target="_blank">Department of Computer and Information Sciences (CIS)</a></span>,&nbsp;<span><a href="https://dsi.udel.edu/" style="color: var(--bs-white);" target="_blank">Data Science Institute (DSI)</a></span>, and&nbsp;<span><a href="https://www.denin.udel.edu/" style="color: var(--bs-white);" target="_blank">Delaware Environmental Institute (DENIN)</a></span>, all at the University of Delaware. <br><br>We work in the area of&nbsp;<span style="font-weight: bold;font-style: italic;font-family: Montserrat, sans-serif;">Deep&nbsp;Learning</span>, <span style="font-weight: bold;font-style: italic;">Machine Learning</span>, and <span style="font-weight: bold;font-style: italic;">Computer Vision</span>, with a special interest in three directions:&nbsp;</p>
                <ul style="font-size: calc(7pt + 0.6vw);">
                    <li style="font-size: calc(10pt + 0.3vw);text-align: left;"><a href="#research_highlights" style="color: var(--bs-white);">Robust &amp; Explainable Machine Learning</a></li>
                    <li style="font-size: calc(10pt + 0.3vw);text-align: left;"><a href="#research_highlights" style="color: var(--bs-white);">Scientific Machine Learning</a></li>
                    <li style="font-size: calc(10pt + 0.3vw);text-align: left;"><a href="#research_highlights" style="color: var(--bs-white);">Human-centered Computer Vision</a></li>
                </ul>
                <p style="font-size: calc(10pt + 0.3vw);font-family: Montserrat, sans-serif;text-align: left;">Our mission is to develop flexible, reliable, and explainable AI models, upon which cross-disciplinary research can advance synergistically, including Geography, Hydrology, and Bioinformatics.<br><br>We are publishing in top-ranked conferences (<span style="font-weight: bold;font-style: italic;">NeurIPS, ICLR, CVPR, ICCV, ECCV, AAAI, IJCAI, KDD</span>), leading journals (<span style="font-weight: bold;font-style: italic;">TPAMI, TIP, IJCV, THMS</span>), as well as US patents. Our current research received awards and&nbsp;supports&nbsp;from <span style="font-weight: bold;font-style: italic;">NSF, CDC, Google Research, Snap Research</span>, and internal sponsors&nbsp;<span style="font-weight: bold;">GUR, UDRF,&nbsp;InAccelerator, DSI</span>.</p><a href="opening.html"><button class="btn btn-outline-light btn-lg" type="button">Prospective Student</button></a>
            </div>
        </section>
        <section class="clean-block clean-gallery dark" style="background: var(--bs-gray-100);">
            <div class="container">
                <div class="block-heading" style="padding-top: 60px;">
                    <h2 class="display-6 text-info">News</h2>
                </div>
                <div class="block-content" style="min-height: 499px;max-height: 500px;"><iframe id="frame_content" src="news.html" width="100%" height="400px"></iframe></div>
            </div>
        </section>
        <section id="research_highlights" class="clean-block clean-gallery dark" style="background: var(--bs-gray-100);">
            <div class="container">
                <div class="block-heading" style="padding-top: 0px;">
                    <h2 class="display-6 text-info">Research Highlights</h2>
                </div>
                <div class="block-content">
                    <div>
                        <ul class="nav nav-tabs" role="tablist">
                            <li class="nav-item" role="presentation" style="background: var(--bs-white);"><a class="nav-link active" role="tab" data-bs-toggle="tab" href="#tab-1" style="font-family: Montserrat, sans-serif;font-size: 20px;background: var(--bs-white);">Robust &amp; Explainable Machine Learning</a></li>
                            <li class="nav-item" role="presentation"><a class="nav-link" role="tab" data-bs-toggle="tab" href="#tab-2" style="font-size: 20px;">Human-centered Computer Vision</a></li>
                        </ul>
                        <div class="tab-content">
                            <div class="tab-pane active" role="tabpanel" id="tab-1">
                                <div class="col">
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Beyond the Federation: Topology-aware Federated Learning for Generalization to Unseen Clients</strong><br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICML'24<strong>]</strong></span>&nbsp;Mengmeng Ma, Tang Li, and Xi Peng.</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of the International Conference on Machine Learning, 2024. (acceptance rate 27.5%)</p>
                                                            <div id="icml24_ma_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="#" style="margin-right: 20px;" target="_blank">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#icml24_ma_abs" aria-expanded="false" aria-controls="icml24_ma_abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#icml24_ma" aria-expanded="false" aria-controls="icml24_ma">BibTeX</button>
                                                                <div id="icml24_ma" class="collapse" data-bs-parent="#icml24_ma_group">
                                                                    <div class="card">
                                                                        <div class="card-body" style="background: var(--bs-light);"><code>@inproceedings{ma2024beyond,<br>&nbsp;title={Beyond the Federation: Topology-aware Federated Learning for Generalization to Unseen Clients},<br>&nbsp;author={Ma, Mengmeng and Li, Tang and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the International Conference on Machine Learning (ICML)},<br>&nbsp;year={2024}<br>}</code></div>
                                                                    </div>
                                                                </div>
                                                                <div id="icml24_ma_abs" class="collapse" data-bs-parent="#icml24_ma_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Federated Learning is widely employed to tackle distributed sensitive data. Existing methods primarily focus on addressing in-federation data heterogeneity. However, we observed that they suffer from significant performance degradation when applied to unseen clients for out-of-federation (OOF) generalization. The recent attempts to address generalization to unseen clients generally struggle to scale up to large-scale distributed settings due to high communication or computation costs. Moreover, methods that scale well often demonstrate poor generalization capability. To achieve OOF-resiliency in a scalable manner, we propose Topology-aware Federated Learning (TFL) that leverages client topology - a graph representing client relationships - to effectively train robust models against OOF data. We formulate a novel optimization problem for TFL, consisting of two key modules: Client Topology Learning, which infers the client relationships in a privacy-preserving manner, and Learning on Client Topology, which leverages the learned topology to identify influential clients and harness this information into the FL optimization process to efficiently build robust models. Empirical evaluation on a variety of real-world datasets verifies TFL’s superior OOF robustness and scalability.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">ICML</p><img src="assets/img/conference/TFL.png">
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Ensemble Pruning for Out-of-distribution Generalization</strong><br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICML'24<strong>]</strong></span>&nbsp;Fengchun Qiao and Xi Peng.</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of the International Conference on Machine Learning, 2024. (acceptance rate 27.5%)</p>
                                                            <div id="icml24_qiao_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="#" style="margin-right: 20px;" target="_blank">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#icml24_qiao_abs" aria-expanded="false" aria-controls="icml24_qiao_abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#icml24_qiao" aria-expanded="false" aria-controls="icml24_qiao">BibTeX</button>
                                                                <div id="icml24_qiao" class="collapse" data-bs-parent="#icml24_qiao_group">
                                                                    <div class="card">
                                                                        <div class="card-body" style="background: var(--bs-light);"><code>@inproceedings{qiao2024ensemble,<br>&nbsp;title={Ensemble Pruning for Out-of-distribution Generalization},<br>&nbsp;author={Qiao, Fengchun and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the International Conference on Machine Learning (ICML)},<br>&nbsp;year={2024}<br>}</code></div>
                                                                    </div>
                                                                </div>
                                                                <div id="icml24_qiao_abs" class="collapse" data-bs-parent="#icml24_qiao_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Ensemble of deep neural networks has achieved great success in hedging against single-model failure under distribution shift. However, existing techniques suffer from producing redundant models, limiting predictive diversity and yielding compromised generalization performance. Existing ensemble pruning methods can only guarantee predictive diversity for in-distribution data, which may not transfer well to out-of-distribution (OoD) data. To address this gap, we propose a principled optimization framework for ensemble pruning under distribution shifts. Since the annotations of test data are not available, we explore relationships between prediction distributions of the models, encapsulated in a topology graph. By incorporating this topology into a combinatorial optimization framework, complementary models with high predictive diversity are selected with theoretical guarantees. Our approach is model-agnostic and can be applied on top of a broad spectrum of off-the-shelf ensembling methods for improved generalization performance. Experiments on common benchmarks demonstrate the superiority of our approach in both multi- and single-source OoD generalization.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">ICML</p><img src="assets/img/conference/TEP.png">
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>DEAL: Disentangle and Localize Concept-level Explanations for VLMs</strong><br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ECCV'24<strong>]</strong></span>&nbsp;Tang Li, Mengmeng Ma, and Xi Peng.</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of the European Conference on Computer Vision (ECCV), 2024. (acceptance rate 27.9%)</p>
                                                            <div id="eccv24_li_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="#" style="margin-right: 20px;" target="_blank">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#eccv24_li_abs" aria-expanded="false" aria-controls="eccv24_li_abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#eccv24_li" aria-expanded="false" aria-controls="eccv24_li">BibTeX</button>
                                                                <div id="eccv24_li" class="collapse" data-bs-parent="#eccv24_li_group">
                                                                    <div class="card">
                                                                        <div class="card-body" style="background: var(--bs-light);"><code>@inproceedings{li2024deal,<br>&nbsp;title={DEAL: Disentangle and Localize Concept-level Explanations for VLMs},<br>&nbsp;author={Li, Tang and Ma, Mengmeng and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},<br>&nbsp;year={2024}<br>}</code></div>
                                                                    </div>
                                                                </div>
                                                                <div id="eccv24_li_abs" class="collapse" data-bs-parent="#eccv24_li_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Large pre-trained Vision-Language Models (VLMs) have become ubiquitous foundational components of other models and downstream tasks. Although powerful, our empirical results reveal that such models might not be able to identify fine-grained concepts. Specifically, the explanations of VLMs with respect to fine-grained concepts are entangled and mislocalized. To address this issue, we propose to DisEntAngle and Localize (DEAL) the concept-level explanations for VLMs without human annotations. The key idea is encouraging the concept-level explanations to be distinct while maintaining consistency with category-level explanations. We conduct extensive experiments and ablation studies on a wide range of benchmark datasets and vision-language models. Our empirical results demonstrate that the proposed method significantly improves the concept-level explanations of the model in terms of disentanglability and localizability. Surprisingly, the improved explainability alleviates the model's reliance on spurious correlations, which further benefits the prediction accuracy.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">ECCV</p><img src="assets/img/conference/DEAL.png">
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Topology-aware Robust Optimization for Out-of-Distribution Generalization</strong><br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICLR'23<strong>]</strong></span>&nbsp;Fengchun Qiao and Xi Peng.</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of the International Conference on Learning Representations, 2023. (acceptance rate 31.8%)</p>
                                                            <div id="iclr23_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openreview.net/pdf?id=ylMq8MBnAp" style="margin-right: 20px;" target="_blank">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#iclr23abs" aria-expanded="false" aria-controls="iclr23abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#iclr23" aria-expanded="false" aria-controls="iclr23">BibTeX</button>
                                                                <div id="iclr23" class="collapse" data-bs-parent="#iclr23_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{qiao2023topology,<br>&nbsp;title={Topology-aware Robust Optimization for Out-of-Distribution Generalization},<br>&nbsp;author={Qiao, Fengchun and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},<br>&nbsp;year={2023}<br>}</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div id="iclr23abs" class="collapse" data-bs-parent="#iclr23_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach, and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.&nbsp;</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">ICLR</p><img src="assets/img/conference/TRO.jpg">
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Are Data-driven Explanations Robust against Out-of-distribution Data?</strong><br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVPR'23<strong>]</strong></span>&nbsp;Tang Li, Fengchun Qiao, Mengmeng Ma, and Xi Peng.</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. (acceptance rate 25.8%)</p>
                                                            <div id="cvpr23_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Are_Data-Driven_Explanations_Robust_Against_Out-of-Distribution_Data_CVPR_2023_paper.pdf" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://www.youtube.com/watch?v=4-8zMdB83x8" style="margin-right: 20px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr23abs" aria-expanded="false" aria-controls="cvpr23abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr23" aria-expanded="false" aria-controls="cvpr23">BibTeX</button>
                                                                <div id="cvpr23" class="collapse" data-bs-parent="#cvpr23_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{tang2023dre,<br>&nbsp;title={Are Data-driven Explanations Robust against Out-of-distribution Data?},<br>&nbsp;author={Li, Tang and Qiao, Fengchun and Ma, Mengmeng and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>&nbsp;year={2023}<br>}</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div id="cvpr23abs" class="collapse" data-bs-parent="#cvpr23_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">As black-box models increasingly power high-stakes applications, a variety of data-driven explanation methods have been introduced. Meanwhile, machine learning models are constantly challenged by distributional shifts. A question naturally arises: Are data-driven explanations robust against out-of-distribution data? Our empirical results show that even though made accurate predictions, the model might still yield unreliable explanations under distribution shifts. How to develop robust explanations against out-of-distribution data? To address this problem, we propose an end-to-end model-agnostic learning framework Distributionally Robust Explanations (DRE). The key idea is, inspired by self-supervised learning, to fully utilizes the inter-distribution information to provide supervisory signals for the learning of explanations without human annotation. Can robust explanations benefit the model’s generalization capability? We conduct extensive experiments on a wide range of tasks and data types, including classification and regression on image and scientific tabular data. Our results demonstrate that the proposed method significantly improves the model performance in terms of explanation and prediction robustness against distributional shifts.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">CVPR</p><img src="assets/img/conference/DRE.jpg">
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Out-of-Domain Generalization from a Single&nbsp;Source: An Uncertainty Quantification Approach</strong><br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[TPAMI'22<strong>]</strong></span>&nbsp;Xi Peng, Fengchun Qiao, and Long Zhao.</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.&nbsp;<span style="font-weight: bold;font-style: italic;color: var(--bs-gray-900);">(Impact Factor 24.3)</span></p>
                                                            <div id="tpami_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://ieeexplore.ieee.org/document/9801711" style="margin-right: 20px;" target="_blank">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#tpami22abs" aria-expanded="false" aria-controls="tpami22abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#tpami22" aria-expanded="false" aria-controls="tpami22">BibTeX</button>
                                                                <div id="tpami22" class="collapse" data-bs-parent="#tpami_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@article{peng2022out,<br>&nbsp;title={Out-of-Domain Generalization From a Single Source: An Uncertainty Quantification Approach},<br>&nbsp;author={Peng, Xi and Qiao, Fengchun and Zhao, Long},<br>&nbsp;journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},<br>&nbsp;year={2022},<br>&nbsp;publisher={IEEE}<br>}</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div id="tpami22abs" class="collapse" data-bs-parent="#tpami_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">We are concerned with a worst-case scenario in model generalization, in the sense that a model aims to perform well on many unseen domains while there is only one single domain available for training. We propose Meta-Learning based Adversarial Domain Augmentation to solve this Out-of-Domain generalization problem. The key idea is to leverage adversarial training to create “fictitious” yet “challenging” populations, from which a model can learn to generalize with theoretical guarantees. To facilitate fast and desirable domain augmentation, we cast the model training in a meta-learning scheme and use a Wasserstein Auto-Encoder to relax the widely used worst-case constraint. We further improve our method by integrating uncertainty quantification for efficient domain generalization. Extensive experiments on multiple benchmark datasets indicate its superior performance in tackling single domain generalization.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">TPAMI</p><img src="assets/img/journal/Out-of-Domain%20Generalization%20from%20a%20Single.jpg">
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Graph-Relational Distributionally Robust Optimization</strong><br></h3>
                                                            <div class="info" style="font-size: calc(8pt + 0.5vw);"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[NeurIPS'22W<strong>]</strong></span>&nbsp;Fengchun Qiao and Xi Peng.</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of the 36th Conference on Neural Information Processing Systems Workshops.</p>
                                                            <div id="neurips22w_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openreview.net/pdf?id=ZwaE7l5NTn" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="#" style="margin-right: 20px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#neurips22wabs" aria-expanded="false" aria-controls="neurips22wabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#neurips22w" aria-expanded="false" aria-controls="neurips22w">BibTeX</button>
                                                                <div id="neurips22w" class="collapse" data-bs-parent="#neurips22w_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{qiao2022graph,<br>&nbsp;title={Graph-Relational Distributionally Robust Optimization},<br>&nbsp;author={Qiao, Fengchun and Peng, Xi},<br>&nbsp;booktitle={NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications}<br>}</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div id="neurips22wabs" class="collapse" data-bs-parent="#neurips22w_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Distributionally robust optimization (DRO) is a promising learning paradigm to tackle this challenge but suffers from several limitations. To address this challenge, we propose graph relational distributionally robust optimization that trains OOD-resilient machine learning models by exploiting the graph structure of data distributions. Our approach can uniformly handle both fully-known and partially-known graph structures. Empirical results on both synthetic and real-world datasets demonstrate the effectiveness and flexibility of our method.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">NeurIPS</p><img src="assets/img/conference/GRDRO.jpg">
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Are multimodal transformers robust to missing modality?</strong><br></h3>
                                                            <div class="info" style="font-size: calc(8pt + 0.5vw);"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVPR'22<strong>]</strong></span>&nbsp;Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, and Xi Peng.</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022. (acceptance rate 25.3%) </p>
                                                            <div id="cvpr22meng_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://drive.google.com/file/d/1yZMkuZXEfk_OvPoVI6Ut1e_4MyTm1oLL/view?usp=sharing" style="margin-right: 20px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr22mengabs" aria-expanded="false" aria-controls="cvpr22mengabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr22meng" aria-expanded="false" aria-controls="cvpr22meng">BibTeX</button>
                                                                <div id="cvpr22meng" class="collapse" data-bs-parent="#cvpr22meng_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{ma2022multimodal,<br>&nbsp;title={Are Multimodal Transformers Robust to Missing Modality?},<br>&nbsp;author={Ma, Mengmeng and Ren, Jian and Zhao, Long and Testuggine, Davide and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>&nbsp;pages={18177--18186},<br>&nbsp;year={2022}<br>}</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div id="cvpr22mengabs" class="collapse" data-bs-parent="#cvpr22meng_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">Multimodal data collected from the real world are often imperfect due to missing modalities. Therefore multimodal models that are robust against modal-incomplete data are highly preferred. Recently, Transformer models have shown great success in processing multimodal data. However, existing work has been limited to either architecture designs or pre-training strategies; whether Transformer models are naturally robust against missing-modal data has rarely been investigated. In this paper, we present the firstof-its-kind work to comprehensively investigate the behavior of Transformers in the presence of modal-incomplete data. Unsurprising, we find Transformer models are sensitive to missing modalities while different modal fusion strategies will significantly affect the robustness. What surprised us is that the optimal fusion strategy is dataset dependent even for the same Transformer model; there does not exist a universal strategy that works in general cases. Based on these findings, we propose a principle method to improve the robustness of Transformer models by automatically searching for an optimal fusion strategy regarding input data. Experimental validations on three benchmarks support the superior performance of the proposed method.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">CVPR</p><div class="iframe-container"><iframe width="352" height="198" src="https://drive.google.com/file/d/1yZMkuZXEfk_OvPoVI6Ut1e_4MyTm1oLL/preview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Deep Learning for Spatiotemporal Modeling of Urbanization</strong><br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[NeurIPS'21W]</span>&nbsp;Tang Li, Jing Gao, and Xi Peng.&nbsp;</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);"><span style="font-weight: bold;font-style: italic;color: var(--bs-gray-900);">Best Paper Award</span>&nbsp;of NeurIPS 2021 Machine Learning in Public Health Workshop. </p>
                                                            <div id="neurips21w_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://arxiv.org/pdf/2112.09668.pdf" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://www.youtube.com/watch?v=3VyVck_gv4g" style="margin-right: 20px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#neurips21wabs" aria-expanded="false" aria-controls="neurips21wabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#neurips21w" aria-expanded="false" aria-controls="neurips21w">BibTeX</button><a class="btn btn-outline-primary" role="button" href="https://sites.google.com/nyu.edu/mlph2021/accepted-papers?authuser=0" style="margin-right: 20px;" target="_blank">Press</a>
                                                                <div id="neurips21w" class="collapse" data-bs-parent="#neurips21w_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@article{li2021deep,<br>title={Deep Learning for Spatiotemporal Modeling of Urbanization},<br>author={Tang Li and Gao, Jing and Xi Peng},<br>journal={Advances in Neural Information Processing Systems Workshops (Best Paper Award)},<br>year={2021}<br>}</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div id="neurips21wabs" class="collapse" data-bs-parent="#neurips21w_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">Urbanization has a strong impact on the health and wellbeing of populations across the world. Predictive spatial modeling of urbanization therefore can be a useful tool for effective public health planning. Many spatial urbanization models have been developed using classic machine learning and numerical modeling techniques. However, deep learning with its proven capacity to capture complex spatiotemporal phenomena has not been applied to urbanization modeling. Here we explore the capacity of deep spatial learning for the predictive modeling of urbanization. We treat numerical geospatial data as images with pixels and channels, and enrich the dataset by augmentation, in order to leverage the high capacity of deep learning. Our resulting model can generate end-to-end multi-variable urbanization predictions, and outperforms a state-of-the-art classic machine learning urbanization model in preliminary comparisons.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">NeurIPS</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/3VyVck_gv4g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);">Uncertainty-guided Model Generalization to Unseen Domains<br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVPR'21]</span>&nbsp;Fengchun Qiao and Xi Peng.&nbsp;</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021. (acceptance rate 23.4%) </p>
                                                            <div id="cvpr21fengchun_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://arxiv.org/pdf/2103.07531.pdf" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://www.youtube.com/watch?v=s9e-n3LOk8Q" style="margin-right: 20px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr21fengchunabs" aria-expanded="false" aria-controls="cvpr21fengchunabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr21fengchun" aria-expanded="false" aria-controls="cvpr21fengchun">BibTeX</button>
                                                                <div id="cvpr21fengchun" class="collapse" data-bs-parent="#cvpr21fengchun_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{qiao2021uncertainty,<br>&nbsp;title={Uncertainty-guided model generalization to unseen domains},<br>&nbsp;author={Qiao, Fengchun and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>&nbsp;pages={6790--6800},<br>&nbsp;year={2021}<br>}</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div id="cvpr21fengchunabs" class="collapse" data-bs-parent="#cvpr21fengchun_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">We study a worst-case scenario in generalization: Out-of-domain generalization from a single source. The goal is to learn a robust model from a single source and expect it to generalize over many unknown distributions. This challenging problem has been seldom investigated while existing solutions suffer from various limitations. In this paper, we propose a new solution. The key idea is to augment the source capacity in both input and label spaces, while the augmentation is guided by uncertainty assessment. To the best of our knowledge, this is the first work to (1) access the generalization uncertainty from a single source and (2) leverage it to guide both input and label augmentation for robust generalization. The model training and deployment are effectively organized in a Bayesian meta-learning framework. We conduct extensive comparisons and ablation study to validate our approach. The results prove our superior performance in a wide scope of tasks including image classification, semantic segmentation, text classification, and speech recognition.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">CVPR</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/s9e-n3LOk8Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>SMIL: Multimodal Learning with Severely Missing Modality</strong><br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[AAAI'21]</span>&nbsp;Mengmeng Ma, Jian Ren, Long Zhao,&nbsp;Sergey Tulyakov, Cathy Wu, Xi Peng.&nbsp;</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In&nbsp;Proceedings of the Association for the Advancement of Artificial Intelligence, 2020. (acceptance rate 21%) </p>
                                                            <div id="aaai21_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://www.aaai.org/AAAI21Papers/AAAI-437.MaM.pdf" style="margin-right: 20px;margin-left: 10px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://www.youtube.com/watch?v=1_yRw_W8KTc" style="margin-right: 20px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#aaai21abs" aria-expanded="false" aria-controls="aaai21abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#aaai21" aria-expanded="false" aria-controls="aaai21">BibTeX</button>
                                                                <div id="aaai21" class="collapse" data-bs-parent="#aaai21_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{ma2021smil,<br>&nbsp;title={SMIL: Multimodal learning with severely missing modality},<br>&nbsp;author={Ma, Mengmeng and Ren, Jian and Zhao, Long and Tulyakov, Sergey and Wu, Cathy and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>&nbsp;volume={35},<br>&nbsp;number={3},<br>&nbsp;pages={2302--2310},<br>&nbsp;year={2021}<br>}</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div id="aaai21abs" class="collapse" data-bs-parent="#aaai21_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">A common assumption in multimodal learning is the completeness of training data, i.e. full modalities are available in all training examples. Although there exists research endeavor in developing novel methods to tackle the incompleteness of testing data, e.g., modalities are partially missing in testing examples, few of them can handle incomplete training modalities. The problem becomes even more challenging if considering the case of severely missing, e.g., 90% training examples may have incomplete modalities. For the first time in the literature, this paper formally studies multimodal learning with missing modality in terms of flexibility (missing modalities in training, testing, or both) and efficiency (most training data have incomplete modality). Technically, we propose a new method named SMIL that leverages Bayesian meta-learning in uniformly achieving both objectives. To validate our idea, we conduct a series of experiments on three popular benchmarks: MM-IMDb, CMUMOSI, and avMNIST. The results prove the state-of-the-art performance of SMIL over existing methods and generative baselines including autoencoders and generative adversarial networks.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">AAAI</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/75P535ob5-Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Learning to Learn Single Domain Generalization</strong><br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVPR'20]</span>&nbsp;Fengchun Qiao, Long Zhao, and Xi Peng.</span></div>
                                                            <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. (acceptance rate 22%) </p>
                                                            <div id="cvpr20fengchun_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://arxiv.org/pdf/2003.13216.pdf" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://youtu.be/ux4LkEZGPyw" style="margin-right: 20px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr20fengchunabs" aria-expanded="false" aria-controls="cvpr20fengchunabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr20fengchun" aria-expanded="false" aria-controls="cvpr20fengchun">BibTeX</button>
                                                                <div id="cvpr20fengchun" class="collapse" data-bs-parent="#cvpr20fengchun_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{qiao2020learning,<br>&nbsp;title={Learning to learn single domain generalization},<br>&nbsp;author={Qiao, Fengchun and Zhao, Long and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>&nbsp;pages={12556--12565},<br>&nbsp;year={2020}<br>}</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div id="cvpr20fengchunabs" class="collapse" data-bs-parent="#cvpr20fengchun_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">We are concerned with a worst-case scenario in model generalization, in the sense that a model aims to perform well on many unseen domains while there is only one single domain available for training. We propose a new method named adversarial domain augmentation to solve this Out-of-Distribution (OOD) generalization problem. The key idea is to leverage adversarial training to create “fictitious” yet “challenging” populations, from which a model can learn to generalize with theoretical guarantees. To facilitate fast and desirable domain augmentation, we cast the model training in a meta-learning scheme and use a Wasserstein Auto-Encoder (WAE) to relax the widely used worst-case constraint. Detailed theoretical analysis is provided to testify our formulation, while extensive experiments on multiple benchmark datasets indicate its superior performance in tackling single domain generalization.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">CVPR</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/ux4LkEZGPyw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="row" style="margin-bottom: 20px;">
                                        <div class="col">
                                            <div class="block-content">
                                                <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                    <div class="row">
                                                        <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                            <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Knowledge as Priors: Cross-Modal Knowledge Generalization&nbsp;for Datasets without Superior Knowledge</strong><br></h3>
                                                            <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVPR'20]</span>&nbsp;Long Zhao, Xi Peng, Yuxiao Chen, Mubbasir Kapadia, and Dimitris N Metaxas.</span></div>
                                                            <div id="cvpr20long_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://arxiv.org/pdf/2004.00176.pdf" style="margin-right: 20px;" target="_blank">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr20longabs" aria-expanded="false" aria-controls="cvpr20longabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr20long" aria-expanded="false" aria-controls="cvpr20long">BibTeX</button>
                                                                <div id="cvpr20long" class="collapse" data-bs-parent="#cvpr20long_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{zhao2020knowledge,<br>&nbsp;title={Knowledge as priors: Cross-modal knowledge generalization for datasets without superior knowledge},<br>&nbsp;author={Zhao, Long and Peng, Xi and Chen, Yuxiao and Kapadia, Mubbasir and Metaxas, Dimitris N},<br>&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>&nbsp;pages={6528--6537},<br>&nbsp;year={2020}<br>}</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div id="cvpr20longabs" class="collapse" data-bs-parent="#cvpr20long_group">
                                                                    <div class="card">
                                                                        <div class="card-body">
                                                                            <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">Cross-modal knowledge distillation deals with transferring knowledge from a model trained with superior modalities (Teacher) to another model trained with weak modalities (Student). Existing approaches require paired training examples exist in both modalities. However, accessing the data from superior modalities may not always be feasible. For example, in the case of 3D hand pose estimation, depth maps, point clouds, or stereo images usually capture better hand structures than RGB images, but most of them are expensive to be collected. In this paper, we propose a novel scheme to train the Student in a Target dataset where the Teacher is unavailable. Our key idea is to generalize the distilled cross-modal knowledge learned from a Source dataset, which contains paired examples from both modalities, to the Target dataset by modeling knowledge as priors on parameters of the Student. We name our method “CrossModal Knowledge Generalization” and demonstrate that our scheme results in competitive performance for 3D hand pose estimation on standard benchmark datasets.</p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                        <div class="col-lg-5 col-xxl-4">
                                                            <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">CVPR</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/PodKLiW5AYM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="row">
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">Adaptive Data Augmentation&nbsp;<span style="font-weight: bold;">[ICCV'19,CVPR'18]</span></p><img src="assets/img/home/jointly.png" style="width: 100%;height: 100%;">
                                        </div>
                                    </div>
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">GCN Representation Learning&nbsp;<span style="font-weight: bold;">[NeurIPS'19,KDD'19]</span> </p><img src="assets/img/home/rethinking.png" style="width: 100%;height: 100%;">
                                        </div>
                                    </div>
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 13px;text-align: center;">Zero-shot Recognition&nbsp;<span style="font-weight: bold;">[NeurIPS'19,CVPR'18]</span></p><img src="assets/img/home/EmbeddedImage.jpeg" style="width: 100%;height: 100%;">
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="tab-pane" role="tabpanel" id="tab-2">
                                <div class="row" style="margin-bottom: 20px;">
                                    <div class="col">
                                        <div class="block-content">
                                            <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                <div class="row">
                                                    <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                        <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition</strong><br></h3>
                                                        <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICCV'23<strong>]</strong></span>&nbsp;Qitong Wang, Long Zhao, Liangzhe Yuan, Ting Liu, and Xi Peng.</span></div>
                                                        <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of IEEE International Conference on Computer Vision (ICCV) 2023. (acceptance rate 26.8%) </p>
                                                        <div id="iccv23qitong_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://arxiv.org/pdf/2308.11489.pdf" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://github.com/wqtwjt1996/SUM-L" style="margin-right: 20px;" target="_blank">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#iccv23qitongabs" aria-expanded="false" aria-controls="iccv23qitongabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#iccv23qitong" aria-expanded="false" aria-controls="iccv23qitong">BibTeX</button>
                                                            <div id="iccv23qitong" class="collapse" data-bs-parent="#iccv23qitong_group">
                                                                <div class="card">
                                                                    <div class="card-body" style="background: var(--bs-light);"><code>@inproceedings{Wang2023LearningFS,<br> title={Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition},<br> author={Qitong Wang and Long Zhao and Liangzhe Yuan and Ting Liu and Xi Peng},<br>&nbsp;booktitle={Proceedings of the International Conference on Computer Vision (ICCV)},<br> year={2023},<br> url={https://api.semanticscholar.org/CorpusID:261064926}}<br>}</code></div>
                                                                </div>
                                                            </div>
                                                            <div id="iccv23qitongabs" class="collapse" data-bs-parent="#iccv23qitong_group">
                                                                <div class="card">
                                                                    <div class="card-body">
                                                                        <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">We are concerned with a challenging scenario in unpaired multiview video learning. In this case, the model aims to learn comprehensive multiview representations while the cross-view semantic information exhibits variations. We propose Semantics-based Unpaired Multiview Learning (SUM-L) to tackle this unpaired multiview learning problem. The key idea is to build cross-view pseudopairs and do view-invariant alignment by leveraging the semantic information of videos. To facilitate the data efficiency of multiview learning, we further perform video-text alignment for first-person and third-person videos, to fully leverage the semantic knowledge to improve video representations. Extensive experiments on multiple benchmark datasets verify the effectiveness of our framework. Our method also outperforms multiple existing view-alignment methods, under the more challenging scenario than typical paired or unpaired multimodal or multiview learning.</p>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-lg-5 col-xxl-4">
                                                        <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">ICCV</p><img class="rounded img-fluid" src="assets/img/conference/ICCV23_Qitong.png" style="width: 100%;">
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="row" style="margin-bottom: 20px;">
                                    <div class="col">
                                        <div class="block-content">
                                            <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                <div class="row">
                                                    <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                        <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Deep learning-based estimation of whole-body kinematics from multi-view images</strong><br></h3>
                                                        <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVIU'23<strong>]</strong></span>&nbsp;Kien X. Nguyen, Liying Zheng, Ashley L. Hawke, Robert E. Carey, Scott P. Breloff, Kang Li, and Xi Peng.</span></div>
                                                        <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">Computer Vision and Image Understanding, 2023.&nbsp;<span style="color: var(--bs-body-color);font-weight: bold;font-style: italic;">(impact factor 4.9)</span></p>
                                                        <div id="cviu23kien_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://arxiv.org/pdf/2307.05896.pdf" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://github.com/Nyquixt/KinematicNet" style="margin-right: 20px;" target="_blank">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cviu23kienabs" aria-expanded="false" aria-controls="cviu23kienabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cviu23kien" aria-expanded="false" aria-controls="cviu23kien">BibTeX</button>
                                                            <div id="cviu23kien" class="collapse" data-bs-parent="#cviu23kien_group">
                                                                <div class="card">
                                                                    <div class="card-body" style="background: var(--bs-light);"><code>@article{nguyen2023deep,<br>&nbsp;title={Deep learning-based estimation of whole-body kinematics from multi-view images},<br>&nbsp;author={Nguyen, Kien X and Zheng, Liying and Hawke, Ashley L and Carey, Robert E and Breloff, Scott P and Li, Kang and Peng, Xi},<br>&nbsp;journal={Computer Vision and Image Understanding},<br>&nbsp;volume={235},<br>&nbsp;pages={103780},<br>&nbsp;year={2023},<br>&nbsp;publisher={Elsevier}<br>}</code></div>
                                                                </div>
                                                            </div>
                                                            <div id="cviu23kienabs" class="collapse" data-bs-parent="#cviu23kien_group">
                                                                <div class="card">
                                                                    <div class="card-body">
                                                                        <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">It is necessary to analyze the whole-body kinematics (including joint locations and joint angles) to assess risks of fatal and musculoskeletal injuries in occupational tasks. Human pose estimation has gotten more attention in recent years as a method to minimize the errors in determining joint locations. However, the joint angles are not often estimated, nor is the quality of joint angle estimation assessed. In this paper, we presented an end-to-end approach on direct joint angle estimation from multi-view images. Our method leveraged the volumetric pose representation and mapped the rotation representation to a continuous space where each rotation was uniquely represented. We also presented a new kinematic dataset in the domain of residential roofing with a data processing pipeline to generate necessary annotations for the supervised training procedure on direct joint angle estimation. We achieved a mean angle error of 7.19◦ on the new Roofing dataset and 8.41◦ on the Human3.6M dataset, paving the way for employment of on-site kinematic analysis using multi-view images.</p>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-lg-5 col-xxl-4">
                                                        <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">CVIU</p><img class="rounded img-fluid" src="assets/img/conference/CVIU23_Kien.png" style="width: 100%;">
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="row" style="margin-bottom: 20px;">
                                    <div class="col">
                                        <div class="block-content">
                                            <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                <div class="row">
                                                    <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                        <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Symmetry and Uncertainty-Aware Object SLAM for 6DoF Object Pose Estimation</strong><br></h3>
                                                        <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVPR'22<strong>]</strong></span>&nbsp;Nathaniel Merrill, Yuliang Guo, Xingxing Zuo, Xinyu Huang, Stefan Leutenegger, Xi Peng, Liu Ren, and Guoquan Huang.</span></div>
                                                        <p style="font-size: calc(8pt + 0.3vw);color: var(--bs-gray-600);">In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022. (acceptance rate 25.3%) </p>
                                                        <div id="cvpr22nate_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Merrill_Symmetry_and_Uncertainty-Aware_Object_SLAM_for_6DoF_Object_Pose_Estimation_CVPR_2022_paper.pdf" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://github.com/rpng/suo_slam" style="margin-right: 20px;" target="_blank">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr22nateabs" aria-expanded="false" aria-controls="cvpr22nateabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr22nate" aria-expanded="false" aria-controls="cvpr22nate">BibTeX</button>
                                                            <div id="cvpr22nate" class="collapse" data-bs-parent="#cvpr22nate_group">
                                                                <div class="card">
                                                                    <div class="card-body">
                                                                        <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{merrill2022symmetry,<br>&nbsp;title={Symmetry and Uncertainty-Aware Object SLAM for 6DoF Object Pose Estimation},<br>&nbsp;author={Merrill, Nathaniel and Guo, Yuliang and Zuo, Xingxing and Huang, Xinyu and Leutenegger, Stefan and Peng, Xi and Ren, Liu and Huang, Guoquan},<br>&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>&nbsp;pages={14901--14910},<br>&nbsp;year={2022}<br>}</p>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                            <div id="cvpr22nateabs" class="collapse" data-bs-parent="#cvpr22nate_group">
                                                                <div class="card">
                                                                    <div class="card-body">
                                                                        <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">We propose a keypoint-based object-level SLAM framework that can provide globally consistent 6DoF pose estimates for symmetric and asymmetric objects alike. To the best of our knowledge, our system is among the first to utilize the camera pose information from SLAM to provide prior knowledge for tracking keypoints on symmetric objects – ensuring that new measurements are consistent with the current 3D scene. Moreover, our semantic keypoint network is trained to predict the Gaussian covariance for the keypoints that captures the true error of the prediction, and thus is not only useful as a weight for the residuals in the system’s optimization problems, but also as a means to detect harmful statistical outliers without choosing a manual threshold. Experiments show that our method provides competitive performance to the state of the art in 6DoF object pose estimation, and at a real-time speed</p>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-lg-5 col-xxl-4">
                                                        <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">CVPR</p><img class="rounded img-fluid" src="assets/img/conference/Symmetry%20and%20Uncertainty-Aware.png" style="width: 100%;">
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="row" style="margin-bottom: 20px;">
                                    <div class="col">
                                        <div class="block-content">
                                            <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                <div class="row">
                                                    <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                        <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>Learning View-Disentangled Human Pose Representation by Contrastive&nbsp;Cross-View Mutual Information Maximization</strong><br></h3>
                                                        <div class="info" style="font-size: calc(8pt + 0.5vw);"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVPR'21 Oral]</span>&nbsp;Long Zhao, Yuxiao Wang, Jiaping Zhao, Liangzhe Yuan, Jennifer J Sun, Florian Schroff, Hartwig Adam, Xi Peng, Dimitris Metaxas, Ting Liu.</span></div>
                                                        <div id="cvpr21long_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://arxiv.org/pdf/2012.01405.pdf" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://www.youtube.com/watch?v=1_yRw_W8KTc" style="margin-right: 20px;" target="_blank">Video</a><a class="btn btn-outline-primary" role="button" href="https://github.com/google-research/google-research/tree/master/poem" style="margin-right: 20px;" target="_blank">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr21longabs" aria-expanded="false" aria-controls="cvpr21longabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#cvpr21long" aria-expanded="false" aria-controls="cvpr21long">BibTeX</button>
                                                            <div id="cvpr21long" class="collapse" data-bs-parent="#cvpr21long_group">
                                                                <div class="card">
                                                                    <div class="card-body">
                                                                        <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{zhao2021learning,<br>&nbsp;title={Learning view-disentangled human pose representation by contrastive cross-view mutual information maximization},<br>&nbsp;author={Zhao, Long and Wang, Yuxiao and Zhao, Jiaping and Yuan, Liangzhe and Sun, Jennifer J and Schroff, Florian and Adam, Hartwig and Peng, Xi and Metaxas, Dimitris and Liu, Ting},<br>&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>&nbsp;pages={12793--12802},<br>&nbsp;year={2021}<br>}</p>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                            <div id="cvpr21longabs" class="collapse" data-bs-parent="#cvpr21long_group">
                                                                <div class="card">
                                                                    <div class="card-body">
                                                                        <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">We introduce a novel representation learning method to disentangle pose-dependent as well as view-dependent factors from 2D human poses. The method trains a network using cross-view mutual information maximization (CV-MIM) which maximizes mutual information of the same pose performed from different viewpoints in a contrastive learning manner. We further propose two regularization terms to ensure disentanglement and smoothness of the learned representations. The resulting pose representations can be used for cross-view action recognition. To evaluate the power of the learned representations, in addition to the conventional fully-supervised action recognition settings, we introduce a novel task called singleshot cross-view action recognition. This task trains models with actions from only one single viewpoint while models are evaluated on poses captured from all possible viewpoints. We evaluate the learned representations on standard benchmarks for action recognition, and show that (i) CV-MIM performs competitively compared with the state-of-the-art models in the fully-supervised scenarios; (ii) CV-MIM outperforms other competing methods by a large margin in the single-shot cross-view setting; (iii) and the learned representations can significantly boost the performance when reducing the amount of supervised training data.</p>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-lg-5 col-xxl-4">
                                                        <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">CVPR</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/1_yRw_W8KTc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="row" style="margin-bottom: 20px;">
                                    <div class="col">
                                        <div class="block-content">
                                            <div class="clean-blog-post" style="padding-bottom: 0px;">
                                                <div class="row">
                                                    <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                                        <h3 style="font-family: Adamina, serif;font-size: calc(9pt + 0.7vw);"><strong>A Good Image Generator Is What You Need for High-Resolution Video Synthesis</strong><br></h3>
                                                        <div class="info"><span style="font-size: calc(8pt + 0.5vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICLR'21 Spotlight]</span>&nbsp;Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, Sergey Tulyakov.</span></div>
                                                        <div id="iclr21_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openreview.net/forum?id=6puCSjH3hwA" style="margin-right: 20px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://papertalk.org/papertalks/29015" style="margin-right: 20px;" target="_blank">Video</a><a class="btn btn-outline-primary" role="button" href="https://github.com/snap-research/MoCoGAN-HD" style="margin-right: 20px;" target="_blank">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#iclr21abs" aria-expanded="false" aria-controls="iclr21abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 20px;" data-bs-toggle="collapse" data-bs-target="#iclr21" aria-expanded="false" aria-controls="iclr21">BibTeX</button>
                                                            <div id="iclr21" class="collapse" data-bs-parent="#iclr21_group">
                                                                <div class="card">
                                                                    <div class="card-body">
                                                                        <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{tian2020good,<br>&nbsp;title={A Good Image Generator Is What You Need for High-Resolution Video Synthesis},<br>&nbsp;author={Tian, Yu and Ren, Jian and Chai, Menglei and Olszewski, Kyle and Peng, Xi and Metaxas, Dimitris N and Tulyakov, Sergey},<br>&nbsp;booktitle={International Conference on Learning Representations},<br>&nbsp;year={2020}<br>}</p>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                            <div id="iclr21abs" class="collapse" data-bs-parent="#iclr21_group">
                                                                <div class="card">
                                                                    <div class="card-body">
                                                                        <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques.</p>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="col-lg-5 col-xxl-4">
                                                        <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">ICLR</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/bucl2kXMbSM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="row" style="font-size: 11px;">
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">Roofing: Multi-modal Slip, Trap, Near-fall Detection</p><img src="assets/img/home/roofing.jpg">
                                        </div>
                                    </div>
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">Lifting: AI-based Musculoskeletal Analysis</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/CaweFI90ScI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                        </div>
                                    </div>
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">ASL: Vision-based Translation and Generation</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/aSCzDCUl-Z8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                        </div>
                                    </div>
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">3D Facial Modeling&nbsp;<span style="font-weight: bold;">[ECCV'18,ICCV'17]</span> </p><img src="assets/img/home/facelandmark.png" style="width: 100%;height: 167px;">
                                        </div>
                                    </div>
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">View-Disentangled Pose Repres.&nbsp;<span style="font-weight: bold;">[CVPR'21 Oral]</span></p><img src="assets/img/home/humanpose.png" style="width: 100%;min-width: 190px;height: 167px;">
                                        </div>
                                    </div>
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">View-Disentangled Pose Repres.&nbsp;<span style="font-weight: bold;">[CVPR'21 Oral]</span></p><img src="assets/img/home/handpose.png" style="width: 100%;height: 167px;">
                                        </div>
                                    </div>
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">Facial Landmark Tracking&nbsp;<span style="font-weight: bold;">[ECCV'16]</span></p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/cPF80DXj_3o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                        </div>
                                    </div>
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">Multiview Generation&nbsp;<span style="font-weight: bold;">[IJCAI'18]</span></p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/fyUTne2B0QM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                        </div>
                                    </div>
                                    <div class="col-md-6 col-lg-4 item">
                                        <div class="block-content">
                                            <p style="font-size: 12px;text-align: center;">Attribute Manipulation&nbsp;<span style="font-weight: bold;">[IJCAI'18]</span></p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/C0nnJgv6wJQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="clean-block clean-gallery dark" style="background: var(--bs-white);padding-bottom: 50px;">
            <div class="container">
                <div class="block-heading" style="padding-top: 0px;">
                    <h2 class="text-info" style="padding-top: 30px;">Sponsors</h2>
                </div>
                <div class="row">
                    <div class="col-md-6 col-lg-4 col-xxl-2 item" style="width: 100%;"><img src="assets/img/sponsor/NSF.jpeg" style="width: 9.2%;margin: 0.4%;"><img src="assets/img/sponsor/dod.png" style="width: 9.2%;margin: 0.4%;padding: 1.5%;"><img src="assets/img/sponsor/cdc.jpeg" style="width: 9.2%;margin: 0.4%;"><img src="assets/img/sponsor/MSKCC.png" style="width: 9.2%;margin: 0.4%;"><img src="assets/img/sponsor/Google-research.jpeg" style="width: 9.2%;margin: 0.4%;"><img src="assets/img/sponsor/snapchat.png" style="width: 9.2%;margin: 0.4%;padding: 2%;"><img src="assets/img/sponsor/gur.png" style="width: 9.2%;margin: 0.4%;"><img src="assets/img/sponsor/udrf.png" style="width: 9.2%;margin: 0.4%;"><img src="assets/img/sponsor/udrf-si.png" style="width: 9.2%;margin: 0.4%;"><img src="assets/img/sponsor/aicoe.png" style="width: 9.2%;margin: 0.4%;"></div>
                </div>
            </div>
        </section>
    </main><button class="btn btn-light" data-bss-hover-animate="pulse" id="myBtn" title="Go To Top" onclick="topFunction()" type="button" style="background: rgba(164,164,164,0.42);"><i class="fa fa-chevron-up" style="margin-left: 5px;"></i></button>
    <footer class="page-footer dark">
        <div class="footer-copyright">
            <p>Deep-REAL © 2022 Copyright&nbsp; |&nbsp; designed by <span style="font-style: italic;">Tang Li</span></p>
        </div>
    </footer>
    <script src="assets/bootstrap/js/bootstrap.min.js"></script>
    <script src="assets/js/Back-To-Top.js"></script>
    <script src="assets/js/back-to-top-button.js"></script>
    <script src="assets/js/back-to-top-scroll.js"></script>
    <script src="assets/js/bs-init.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/aos/2.3.4/aos.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/baguettebox.js/1.10.0/baguetteBox.min.js"></script>
    <script src="assets/js/vanilla-zoom.js"></script>
    <script src="assets/js/theme.js"></script>
    <script src="assets/js/Bold-BS4-Animated-Back-To-Top.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Swiper/6.4.8/swiper-bundle.min.js"></script>
    <script src="assets/js/MUSA_back-to-top.js"></script>
    <script src="assets/js/Popup-Element-Overlay.js"></script>
    <script src="assets/js/responsive-blog-card-slider-1.js"></script>
    <script src="assets/js/responsive-blog-card-slider.js"></script>
    <script src="assets/js/Simple-Slider.js"></script>
    <script src="assets/js/Swiper-Slider-Card-For-Blog-Or-Product.js"></script>
    <script src="assets/js/Tabbed_slider.js"></script>
    <script src="assets/js/WOWSlider-about-us.js"></script>
</body>

</html>