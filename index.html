<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>DeepREAL</title>
    <meta name="keywords" content="Deep-REAL, deep-real, udel, UD, university of delaware, Deep Robust &amp; Explainable AI Lab, computer vision, machine learning, xi peng, xipeng, Xi Peng, deepreal">
    <meta name="description" content="Deep Robust &amp; Explainable AI Lab. Deep-REAL is a research lab affiliated with the Department of Computer and Information Sciences and Data Science Institute, at the University of Delaware.">
    <link rel="icon" type="image/png" sizes="553x553" href="assets/img/favicon_circle.png">
    <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,700,700i,600,600i&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Actor&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Adamina&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alatsi&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Aldrich&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alegreya&amp;display=swap">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,400i,700,700i&amp;display=swap">
    <link rel="stylesheet" href="assets/css/Times%20New%20Roman%20Cyr.css">
    <link rel="stylesheet" href="assets/fonts/font-awesome.min.css">
    <link rel="stylesheet" href="assets/css/animate.min.css">
    <link rel="stylesheet" href="assets/css/back-to-top-button.css">
    <link rel="stylesheet" href="assets/css/Back-To-Top.css">
    <link rel="stylesheet" href="assets/css/back-to-top-scroll-1.css">
    <link rel="stylesheet" href="assets/css/back-to-top-scroll.css">
    <link rel="stylesheet" href="assets/css/Bold-BS4-Animated-Back-To-Top.css">
    <link rel="stylesheet" href="assets/css/card-3-column-animation-shadows-images.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/baguettebox.js/1.10.0/baguetteBox.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Swiper/6.4.8/swiper-bundle.min.css">
    <link rel="stylesheet" href="assets/css/MUSA_back-to-top-1.css">
    <link rel="stylesheet" href="assets/css/MUSA_back-to-top.css">
    <link rel="stylesheet" href="assets/css/Navigation-Clean.css">
    <link rel="stylesheet" href="assets/css/Popup-Element-Overlay-1.css">
    <link rel="stylesheet" href="assets/css/Popup-Element-Overlay.css">
    <link rel="stylesheet" href="assets/css/responsive-blog-card-slider.css">
    <link rel="stylesheet" href="assets/css/Simple-Slider.css">
    <link rel="stylesheet" href="assets/css/Swiper-Slider-Card-For-Blog-Or-Product-1.css">
    <link rel="stylesheet" href="assets/css/Swiper-Slider-Card-For-Blog-Or-Product.css">
    <link rel="stylesheet" href="assets/css/Tabbed_slider-1.css">
    <link rel="stylesheet" href="assets/css/Tabbed_slider.css">
    <link rel="stylesheet" href="assets/css/Ultimate-Testimonial-Slider.css">
    <link rel="stylesheet" href="assets/css/untitled-1.css">
    <link rel="stylesheet" href="assets/css/vanilla-zoom.min.css">
    <link rel="stylesheet" href="assets/css/WOWSlider-about-us-1.css">
    <link rel="stylesheet" href="assets/css/WOWSlider-about-us-2.css">
    <link rel="stylesheet" href="assets/css/WOWSlider-about-us.css">
</head>

<body>
    <nav class="navbar navbar-light navbar-expand-lg fixed-top bg-white clean-navbar" style="z-index: 10000;">
        <div class="container"><a class="navbar-brand logo" href="https://deep-real.github.io/"><img src="assets/img/logo.png" style="width: 120px;"></a><button data-bs-toggle="collapse" class="navbar-toggler" data-bs-target="#navcol-2"><span class="visually-hidden">Toggle navigation</span><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navcol-2">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link active" href="index.html" style="font-size: 14px;">HOME</a></li>
                    <li class="nav-item dropdown"><a class="nav-link" href="research.html" style="font-size: 14px;">RESEARCH&nbsp;</a>
                        <div class="dropdown-menu"><a class="dropdown-item" href="#">SeafloorAI</a><a class="dropdown-item" href="#">Prostate</a><a class="dropdown-item" href="#">SafeAI</a></div>
                    </li>
                    <li class="nav-item"><a class="nav-link" href="dr_xipeng.html" style="font-size: 14px;">Dr. XI PENG</a></li>
                    <li class="nav-item"><a class="nav-link" href="publication.html" style="font-size: 14px;">PUBLICATION</a></li>
                    <li class="nav-item"><a class="nav-link" href="teaching.html" style="font-size: 14px;">Teaching</a></li>
                    <li class="nav-item"></li>
                </ul>
            </div>
        </div>
    </nav>
    <main class="page landing-page">
        <section data-bss-parallax-bg="true" style="background: url(&quot;assets/img/background.jpeg&quot;) top / cover no-repeat;margin-top: 10px;">
            <div class="container" style="max-width: 1100px;padding: 50px;padding-right: 20px;padding-left: 20px;">
                <div class="row">
                    <div class="col justify-content-center align-content-center" style="text-align: center;padding-right: 20px;padding-left: 20px;">
                        <div class="text" style="background: rgba(255,255,255,0.9);padding: 30px;padding-top: 30px;padding-right: 10px;padding-bottom: 30px;padding-left: 10px;color: #000000;margin-bottom: 25px;max-width: 100%;">
                            <h2 style="text-align: center;font-family: Montserrat, sans-serif;font-size: 40px;margin-top: 0px;margin-bottom: 15pt;">Welcome to DeepREAL</h2>
                            <p style="font-size: 18px;margin-bottom: 22px;">The <strong>Deep</strong> <strong>R</strong>obust &amp; <strong>E</strong>xplainable <strong>A</strong>I <strong>L</strong>ab consists of computer scientists whose mission is to develop safe, reliable, and explainable AI models, upon which cross-disciplinary research can advance synergistically, particularly for high-stake use in science, medicine, and autonomous systems.</p>
                            <p style="font-weight: bold;font-size: 18px;margin-bottom: 0px;">We work in the areas of&nbsp;Machine Learning, Computer Vision, and Safe Learning System.</p>
                        </div>
                    </div>
                </div>
                <div class="row">
                    <div class="col-md-4" style="padding: 20px;padding-top: 15px;text-align: left;">
                        <p style="margin-bottom: 0px;height: 5px;background: var(--bs-primary);"></p>
                        <div style="background: rgba(255,255,255,0.9);min-height: 100%;margin-bottom: 0px;padding: 20px;margin-right: 0px;padding-top: 18px;padding-right: 20px;padding-left: 20px;"><a style="color: #000000;font-size: 24px;" href="#robust_optimization">ROBUST<br>MACHINE LEARNING</a>
                            <p style="color: rgb(0,0,0);text-align: left;font-size: 14px;margin-bottom: 0px;margin-top: 12px;">Tackle the out-of-distribution (OoD) challenge that involves dynamic, long-tail, or previously unseen data. Our works also optimize for modern HPC platform to manage the scaling law.</p>
                        </div>
                    </div>
                    <div class="col-md-4" style="padding: 20px;padding-top: 15px;text-align: left;">
                        <p style="margin-bottom: 0px;height: 5px;background: var(--bs-primary);"></p>
                        <div style="background: rgba(255,255,255,0.9);min-height: 100%;margin-bottom: 0px;padding: 20px;padding-top: 18px;"><a style="color: #000000;font-size: 24px;" href="#xml">EXPLAINABLE MACHINE LEARNING</a>
                            <p style="color: rgb(0,0,0);text-align: left;font-size: 14px;margin-bottom: 0px;margin-top: 12px;">Safeguard AI predictions with valid and transparent rationales for safety and reliabIlity. Our works provide reasons for AI/ML decisions in a way domain experts can understand and can potentially lead to scientific knowledge discovery.</p>
                        </div>
                    </div>
                    <div class="col-md-4" style="padding: 20px;padding-top: 15px;text-align: left;">
                        <p style="margin-bottom: 0px;height: 5px;background: var(--bs-primary);"></p>
                        <div style="background: rgba(255,255,255,0.9);min-height: 100%;margin-bottom: 0px;padding: 20px;padding-top: 18px;"><a style="color: #000000;font-size: 24px;">SAFE LEARNING-ENABLED SYSTEM&nbsp;</a>
                            <p style="color: rgb(0,0,0);text-align: left;font-size: 14px;margin-bottom: 0px;margin-top: 12px;">Develop safe learning systems for critical domains where safety and reliability cannot be compromised. Our group has particular expertise in safe AI for science,&nbsp;medicine, and autonomous systems.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="clean-block clean-gallery dark" style="background: var(--bs-gray-100);">
            <div class="container" style="max-width: 1200px;">
                <div class="block-heading" style="padding-top: 60px;">
                    <h2 class="display-6 text-info">News</h2>
                </div>
                <div class="block-content" style="min-height: 499px;max-height: 900px;padding-right: 0px;padding-left: 0px;padding-top: 20px;padding-bottom: 20px;"><iframe id="frame_content" src="news.html" width="100%" height="600px"></iframe></div>
            </div>
        </section>
        <section id="research_highlights" class="clean-block clean-gallery dark" style="background: var(--bs-gray-100);">
            <div class="container" style="max-width: 1200px;">
                <div class="block-heading" style="padding-top: 0px;">
                    <h2 class="display-6 text-info">Research Highlights</h2>
                </div>
                <div id="robust_optimization" class="block-content" style="margin-bottom: 60px;padding-right: 1%;padding-left: 1%;">
                    <div class="col">
                        <h1 style="font-size: calc(15pt + 0.7vw);text-align: center;margin-bottom: 20px;">Robust Optimization</h1>
                        <div class="clean-blog-post">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Structure-informed Risk Minimization for Robust Ensemble Learning</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICML'25<strong>]</strong></span>&nbsp;Fengchun Qiao, Yanlin Chen and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of the International Conference on Machine Learning, 2025.&nbsp;</p>
                                    <div id="icml25_qiao_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank" href="https://openreview.net/pdf?id=GvV9MJs268">Paper</a><a class="btn btn-outline-primary" role="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank" href="https://github.com/deep-real/SRM">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;margin-left: 0px;width: 95px;" data-bs-target="#icml25_qiao_abs" aria-expanded="false" aria-controls="icml24_qiao_abs" data-bs-toggle="collapse">Abstract</button>
                                        <div id="icml25_qiao" class="collapse" data-bs-parent="#iclr23_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{qiao2024ensemble,<br> title={Ensemble Pruning for Out-of-distribution Generalization},<br> author={Qiao, Fengchun and Peng, Xi},<br> booktitle={Proceedings of the International Conference on Machine Learning (ICML)},<br> year={2024}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="icml25_qiao_abs" class="collapse" data-bs-parent="#icml24_qiao_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Ensemble learning is a powerful approach for improving generalization under distribution shifts, but its effectiveness heavily depends on how individual models are combined. Existing methods often optimize ensemble weights based on validation data, which may not represent unseen test distributions, leading to suboptimal performance in out-of-distribution (OoD) settings. Inspired by Distributionally Robust Optimization (DRO), we propose Structure-informed Risk Minimization (SRM), a principled framework that learns robust ensemble weights without access to test data. Unlike standard DRO, which defines uncertainty sets based on divergence metrics alone, SRM incorporates structural information of training distributions, ensuring that the uncertainty set aligns with plausible real-world shifts. This approach mitigates the over-pessimism of traditional worst-case optimization while maintaining robustness. We introduce a computationally efficient optimization algorithm with theoretical guarantees and demonstrate that SRM achieves superior OoD generalization compared to existing ensemble combination strategies across diverse benchmarks. Code is available at: https://github.com/deep-real/SRM.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">ICML</p><img src="assets/img/conference/Fengchun_ICML25.png">
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Ensemble Pruning for Out-of-distribution Generalization</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICML'24<strong>]</strong></span>&nbsp;Fengchun Qiao and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of the International Conference on Machine Learning, 2024.&nbsp;</p>
                                    <div id="icml24_qiao_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://proceedings.mlr.press/v235/qiao24a.html" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://github.com/joffery/TEP" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;margin-left: 0px;width: 95px;" data-bs-toggle="collapse" data-bs-target="#icml24_qiao_abs" aria-expanded="false" aria-controls="icml24_qiao_abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#icml24_qiao" aria-expanded="false" aria-controls="icml24_ma-2">BibTeX</button>
                                        <div id="icml24_qiao" class="collapse" data-bs-parent="#iclr23_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{qiao2024ensemble,<br> title={Ensemble Pruning for Out-of-distribution Generalization},<br> author={Qiao, Fengchun and Peng, Xi},<br> booktitle={Proceedings of the International Conference on Machine Learning (ICML)},<br> year={2024}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="icml24_qiao_abs" class="collapse" data-bs-parent="#icml24_qiao_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Ensemble of deep neural networks has achieved great success in hedging against single-model failure under distribution shift. However, existing techniques suffer from producing redundant models, limiting predictive diversity and yielding compromised generalization performance. Existing ensemble pruning methods can only guarantee predictive diversity for in-distribution data, which may not transfer well to out-of-distribution (OoD) data. To address this gap, we propose a principled optimization framework for ensemble pruning under distribution shifts. Since the annotations of test data are not available, we explore relationships between prediction distributions of the models, encapsulated in a topology graph. By incorporating this topology into a combinatorial optimization framework, complementary models with high predictive diversity are selected with theoretical guarantees. Our approach is model-agnostic and can be applied on top of a broad spectrum of off-the-shelf ensembling methods for improved generalization performance. Experiments on common benchmarks demonstrate the superiority of our approach in both multi- and single-source OoD generalization.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">ICML</p><img src="assets/img/conference/TEP.png">
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Beyond the Federation: Topology-aware Federated Learning for Generalization to Unseen Clients</strong><br></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICML'24<strong>]</strong></span>&nbsp;Mengmeng Ma, Tang Li, and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of the International Conference on Machine Learning, 2024.</p>
                                    <div id="icml24_ma_group-2" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://proceedings.mlr.press/v235/ma24e.html" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://github.com/mengmenm/TFL-OOF" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#icml24_ma_abs-2" aria-expanded="false" aria-controls="icml24_ma_abs-2">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#icml24_ma-2" aria-expanded="false" aria-controls="icml24_ma-2">BibTeX</button>
                                        <div id="icml24_ma-2" class="collapse" data-bs-parent="#iclr23_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{ma2024beyond,<br> title={Beyond the Federation: Topology-aware Federated Learning for Generalization to Unseen Clients},<br> author={Ma, Mengmeng and Li, Tang and Peng, Xi},<br> booktitle={Proceedings of the International Conference on Machine Learning (ICML)},<br> year={2024}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="icml24_ma_abs-2" class="collapse" data-bs-parent="#icml24_ma_group-2">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Federated Learning is widely employed to tackle distributed sensitive data. Existing methods primarily focus on addressing in-federation data heterogeneity. However, we observed that they suffer from significant performance degradation when applied to unseen clients for out-of-federation (OOF) generalization. The recent attempts to address generalization to unseen clients generally struggle to scale up to large-scale distributed settings due to high communication or computation costs. Moreover, methods that scale well often demonstrate poor generalization capability. To achieve OOF-resiliency in a scalable manner, we propose Topology-aware Federated Learning (TFL) that leverages client topology - a graph representing client relationships - to effectively train robust models against OOF data. We formulate a novel optimization problem for TFL, consisting of two key modules: Client Topology Learning, which infers the client relationships in a privacy-preserving manner, and Learning on Client Topology, which leverages the learned topology to identify influential clients and harness this information into the FL optimization process to efficiently build robust models. Empirical evaluation on a variety of real-world datasets verifies TFL’s superior OOF robustness and scalability.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">ICML</p><div class="iframe-container"><iframe width="352" height="198" src="https://drive.google.com/file/d/1uPGpFzsk_SgV_MA-qnY9k6QlMoyaTqrD/preview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Topology-aware Robust Optimization for Out-of-Distribution Generalization</strong><br></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICLR'23<strong>]</strong></span>&nbsp;Fengchun Qiao and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of the International Conference on Learning Representations, 2023.</p>
                                    <div id="iclr23_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openreview.net/pdf?id=ylMq8MBnAp" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#iclr23abs" aria-expanded="false" aria-controls="iclr23abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#iclr23" aria-expanded="false" aria-controls="iclr23">BibTeX</button>
                                        <div id="iclr23" class="collapse" data-bs-parent="#iclr23_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{qiao2023topology,<br>&nbsp;title={Topology-aware Robust Optimization for Out-of-Distribution Generalization},<br>&nbsp;author={Qiao, Fengchun and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},<br>&nbsp;year={2023}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="iclr23abs" class="collapse" data-bs-parent="#iclr23_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach, and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.&nbsp;</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">ICLR</p><img src="assets/img/conference/TRO.jpg">
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Out-of-Domain Generalization from a Single&nbsp;Source: An Uncertainty Quantification Approach</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[TPAMI'22<strong>]</strong></span>&nbsp;Xi Peng, Fengchun Qiao, and Long Zhao.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.</p>
                                    <div id="tpami_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://ieeexplore.ieee.org/document/9801711" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#tpami22abs" aria-expanded="false" aria-controls="tpami22abs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#tpami22" aria-expanded="false" aria-controls="tpami22">BibTeX</button>
                                        <div id="tpami22" class="collapse" data-bs-parent="#tpami_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@article{peng2022out,<br>&nbsp;title={Out-of-Domain Generalization From a Single Source: An Uncertainty Quantification Approach},<br>&nbsp;author={Peng, Xi and Qiao, Fengchun and Zhao, Long},<br>&nbsp;journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},<br>&nbsp;year={2022},<br>&nbsp;publisher={IEEE}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="tpami22abs" class="collapse" data-bs-parent="#tpami_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">We are concerned with a worst-case scenario in model generalization, in the sense that a model aims to perform well on many unseen domains while there is only one single domain available for training. We propose Meta-Learning based Adversarial Domain Augmentation to solve this Out-of-Domain generalization problem. The key idea is to leverage adversarial training to create “fictitious” yet “challenging” populations, from which a model can learn to generalize with theoretical guarantees. To facilitate fast and desirable domain augmentation, we cast the model training in a meta-learning scheme and use a Wasserstein Auto-Encoder to relax the widely used worst-case constraint. We further improve our method by integrating uncertainty quantification for efficient domain generalization. Extensive experiments on multiple benchmark datasets indicate its superior performance in tackling single domain generalization.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">TPAMI</p><img src="assets/img/journal/Out-of-Domain%20Generalization%20from%20a%20Single.jpg">
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding: 0px;padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);">Uncertainty-guided Model Generalization to Unseen Domains</h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;font-size: calc(14px + 0.2vw);">[CVPR'21]</span>&nbsp;Fengchun Qiao and Xi Peng.&nbsp;</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021.&nbsp;</p>
                                    <div id="cvpr21fengchun_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Qiao_Uncertainty-Guided_Model_Generalization_to_Unseen_Domains_CVPR_2021_paper.pdf" target="_blank" style="width: 95px;margin-right: 25px;margin-left: 0px;">Paper</a><a class="btn btn-outline-primary" role="button" href="https://www.youtube.com/watch?v=s9e-n3LOk8Q" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#cvpr21fengchunabs" aria-expanded="false" aria-controls="cvpr21fengchunabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#cvpr21fengchun" aria-expanded="false" aria-controls="cvpr21fengchun">BibTeX</button>
                                        <div id="cvpr21fengchun" class="collapse" data-bs-parent="#cvpr21fengchun_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{qiao2021uncertainty,<br>&nbsp;title={Uncertainty-guided model generalization to unseen domains},<br>&nbsp;author={Qiao, Fengchun and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>&nbsp;pages={6790--6800},<br>&nbsp;year={2021}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="cvpr21fengchunabs" class="collapse" data-bs-parent="#cvpr21fengchun_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">We study a worst-case scenario in generalization: Out-of-domain generalization from a single source. The goal is to learn a robust model from a single source and expect it to generalize over many unknown distributions. This challenging problem has been seldom investigated while existing solutions suffer from various limitations. In this paper, we propose a new solution. The key idea is to augment the source capacity in both input and label spaces, while the augmentation is guided by uncertainty assessment. To the best of our knowledge, this is the first work to (1) access the generalization uncertainty from a single source and (2) leverage it to guide both input and label augmentation for robust generalization. The model training and deployment are effectively organized in a Bayesian meta-learning framework. We conduct extensive comparisons and ablation study to validate our approach. The results prove our superior performance in a wide scope of tasks including image classification, semantic segmentation, text classification, and speech recognition.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: 125px;margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: 23px;">CVPR</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/s9e-n3LOk8Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 0px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Learning to Learn Single Domain Generalization</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVPR'20]</span>&nbsp;Fengchun Qiao, Long Zhao, and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.&nbsp;</p>
                                    <div id="cvpr20fengchun_group-1" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_Learning_to_Learn_Single_Domain_Generalization_CVPR_2020_paper.pdf" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://youtu.be/ux4LkEZGPyw" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#cvpr20fengchunabs" aria-expanded="false" aria-controls="cvpr20fengchunabs">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#cvpr20fengchun" aria-expanded="false" aria-controls="cvpr20fengchun">BibTeX</button>
                                        <div id="cvpr20fengchun-1" class="collapse" data-bs-parent="#cvpr20fengchun_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{qiao2020learning,<br>&nbsp;title={Learning to learn single domain generalization},<br>&nbsp;author={Qiao, Fengchun and Zhao, Long and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>&nbsp;pages={12556--12565},<br>&nbsp;year={2020}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="cvpr20fengchunabs-1" class="collapse" data-bs-parent="#cvpr20fengchun_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">We are concerned with a worst-case scenario in model generalization, in the sense that a model aims to perform well on many unseen domains while there is only one single domain available for training. We propose a new method named adversarial domain augmentation to solve this Out-of-Distribution (OOD) generalization problem. The key idea is to leverage adversarial training to create “fictitious” yet “challenging” populations, from which a model can learn to generalize with theoretical guarantees. To facilitate fast and desirable domain augmentation, we cast the model training in a meta-learning scheme and use a Wasserstein Auto-Encoder (WAE) to relax the widely used worst-case constraint. Detailed theoretical analysis is provided to testify our formulation, while extensive experiments on multiple benchmark datasets indicate its superior performance in tackling single domain generalization.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">CVPR</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/ux4LkEZGPyw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div id="xml" class="block-content" style="margin-bottom: 60px;padding-right: 1%;padding-left: 1%;">
                    <div class="col">
                        <h1 style="font-size: calc(15pt + 0.7vw);text-align: center;margin-bottom: 20px;">Rational Prediction</h1>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>"Why Is There a Tumor?": Tell Me the Reason, Show Me the Evidence</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICML'25<strong>]</strong></span>&nbsp;Mengmeng Ma, Tang Li, Yunxiang Peng, Lu Lin, Volkan Beylergil, Binsheng Zhao, Oguz Akin, and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of the International Conference on Machine Learning, 2025.&nbsp;</p>
                                    <div id="icml25_meng" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank" href="https://openreview.net/pdf?id=r3ZLefVUMO">Paper</a><a class="btn btn-outline-primary" role="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank" href="https://github.com/deep-real/MedRationale">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-target="#icml25_meng-abs" aria-expanded="false" aria-controls="eccv24_li_abs-1" data-bs-toggle="collapse">Abstract</button>
                                        <div id="icml25_meng" class="collapse" data-bs-parent="#aaai25_kien">
                                            <div class="card">
                                                <div class="card-body" style="background: var(--bs-light);"><code>@inproceedings{li2024deal,<br>&nbsp;title={DEAL: Disentangle and Localize Concept-level Explanations for VLMs},<br>&nbsp;author={Li, Tang and Ma, Mengmeng and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},<br>&nbsp;year={2024}<br>}</code></div>
                                            </div>
                                        </div>
                                        <div id="icml25_meng-abs" class="collapse" data-bs-parent="#aaai25_kien">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Medical AI models excel at tumor detection and segmentation. However, their latent representations often lack explicit ties to clinical semantics, producing outputs less trusted in clinical practice. Most of the existing models generate either segmentation masks/labels (localizing where without why) or textual justifications (explaining why without where), failing to ground clinical concepts in spatially localized evidence. To bridge this gap, we propose to develop models that can justify the segmentation or detection using clinically relevant terms and point to visual evidence. We address two core challenges: First, we curate a rationale dataset to tackle the lack of paired images, annotations, and textual rationales for training. The dataset includes 180K image-mask-rationale triples with quality evaluated by expert radiologists. Second, we design rationale-informed optimization that disentangles and localizes fine-grained clinical concepts in a self-supervised manner without requiring pixel-level concept annotations. Experiments across medical benchmarks show our model demonstrates superior performance in segmentation, detection, and beyond.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">ICML</p><img src="assets/img/conference/Meng_ICML25.png">
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Interpretable Failure Detection with Human-Level Concepts</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[AAAI'25 Oral<strong>]</strong></span>&nbsp;Kien X. Nguyen, Tang Li, and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of&nbsp;the Association for the Advancement of Artificial Intelligence (AAAI), 2025.</p>
                                    <div id="aaai25_kien" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank" href="https://arxiv.org/pdf/2502.05275">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-target="#aaai25_kien-abs" aria-expanded="false" aria-controls="eccv24_li_abs-1" data-bs-toggle="collapse">Abstract</button>
                                        <div id="aaai25_kien" class="collapse" data-bs-parent="#aaai25_kien">
                                            <div class="card">
                                                <div class="card-body" style="background: var(--bs-light);"><code>@inproceedings{li2024deal,<br>&nbsp;title={DEAL: Disentangle and Localize Concept-level Explanations for VLMs},<br>&nbsp;author={Li, Tang and Ma, Mengmeng and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},<br>&nbsp;year={2024}<br>}</code></div>
                                            </div>
                                        </div>
                                        <div id="aaai25_kien-abs" class="collapse" data-bs-parent="#aaai25_kien">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Reliable failure detection holds paramount importance in safety-critical applications. Yet, neural networks are known to produce overconfident predictions for misclassified samples. As a result, it remains a problematic matter as existing confidence score functions rely on category-level signals, the logits, to detect failures. This research introduces an innovative strategy, leveraging human-level concepts for a dual purpose: to reliably detect <em>when</em> a model fails and to transparently interpret <em>why</em>. By integrating a nuanced array of signals for each category, our method enables a finer-grained assessment of the model’s confidence. We present a simple yet highly effective approach based on the ordinal ranking of concept activation to the input image. Without bells and whistles,<br>our method significantly reduce the false positive rate across diverse real-world image classification benchmarks, specifically by 3.7% on <em>ImageNet</em> and 9% on <em>EuroSAT</em>. Our code is available at https://anonymous.4open.science/r/AAAI-143.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">AAAI</p><img src="assets/img/conference/kien_aaai25.png">
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Beyond Accuracy: On the Effects of Fine-tuning Towards Vision-Language Model's Prediction Rationality</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[AAAI'25<strong>]</strong></span>&nbsp;Qitong Wang,&nbsp;Tang Li,&nbsp;Kien X. Nguyen,&nbsp;Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of&nbsp;the Association for the Advancement of Artificial Intelligence (AAAI), 2025.</p>
                                    <div id="aaai25_wang" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank" href="https://arxiv.org/pdf/2400.1">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-target="#aaai25_wang-abs" aria-expanded="false" aria-controls="eccv24_li_abs-1" data-bs-toggle="collapse">Abstract</button>
                                        <div id="aaai25_wang-bibtex" class="collapse" data-bs-parent="#aaai25_wang">
                                            <div class="card">
                                                <div class="card-body" style="background: var(--bs-light);"><code>@inproceedings{li2024deal,<br>&nbsp;title={DEAL: Disentangle and Localize Concept-level Explanations for VLMs},<br>&nbsp;author={Li, Tang and Ma, Mengmeng and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},<br>&nbsp;year={2024}<br>}</code></div>
                                            </div>
                                        </div>
                                        <div id="aaai25_wang-abs" class="collapse" data-bs-parent="#aaai25_wang">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Vision-Language Models (VLMs), such as CLIP, have already seen widespread applications. Researchers actively engage in further fine-tuning VLMs in safety-critical domains. In these domains, prediction rationality is crucial: the prediction should be correct and based on valid evidence. Yet, for VLMs, the impact of fine-tuning on prediction rationality is seldomly investigated. To study this problem, we proposed two new metrics called Prediction Trustworthiness and Inference Reliability. We conducted extensive experiments on various settings and observed some interesting phenomena. On the one hand, we found that the well-adopted fine-tuning methods led to more correct predictions based on invalid evidence. This potentially undermines the trustworthiness of correct predictions from fine-tuned VLMs. On the other hand, having identified valid evidence of target objects, fine-tuned VLMs were more likely to make correct predictions. Moreover, the findings are also consistent under distributional shifts and across various experimental settings. We hope our research offer fresh insights to VLM fine-tuning.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">AAAI</p><img src="assets/img/conference/ModelSafety.png">
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Beyond Accuracy: Ensuring Correct Predictions with Correct Rationales</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[NeurIPS'24<strong>]</strong></span>&nbsp;Tang Li, Mengmeng Ma, and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of&nbsp;the Annual Conference on Neural Information Processing Systems (NeurIPS), 2024.</p>
                                    <div id="neurips24_li_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank" href="https://arxiv.org/pdf/2411.00132">Paper</a><a class="btn btn-outline-primary" role="button" href="https://github.com/deep-real/DCP" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-target="#neurips24_li_abs" aria-expanded="false" aria-controls="neurips24_li_abs" data-bs-toggle="collapse">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#neurips24_li" aria-expanded="false" aria-controls="eccv24_li-1">BibTeX</button>
                                        <div id="neurips24_li" class="collapse" data-bs-parent="#neurips24_li_group">
                                            <div class="card">
                                                <div class="card-body" style="background: var(--bs-light);">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{libeyond,<br>&nbsp;title={Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales},<br>&nbsp;author={Li, Tang and Ma, Mengmeng and Peng, Xi},<br>&nbsp;booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},<br>&nbsp;year={2024}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="neurips24_li_abs" class="collapse" data-bs-parent="#neurips24_li_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Large pretrained foundation models demonstrate exceptional performance and, in some high-stakes applications, even surpass human experts. However, most of these models are currently evaluated primarily on prediction accuracy, overlooking the validity of the rationales behind their accurate predictions. For the safe deployment of foundation models, there is a pressing need to ensure double-correct predictions, i.e., correct prediction backed by correct rationales. To achieve this, we propose a two-phase scheme: First, we curate a new dataset that offers structured rationales for visual recognition tasks. Second, we propose a rationale-informed optimization<br>method to guide the model in disentangling and localizing visual evidence for each rationale, without requiring manual annotations. Extensive experiments and ablation studies demonstrate that our model outperforms state-of-the-art models by up to 10.1% in prediction accuracy across a wide range of tasks. Furthermore, our method significantly improves the model’s rationale correctness, improving localization by 7.5% and disentanglement by 36.5%. Our dataset, source code, and pretrained weights: https://github.com/deep-real/DCP</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">NeurIPS</p><img src="assets/img/conference/Tang_NeurIPS24.png">
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[NeurIPS'24<strong>]</strong></span>&nbsp;Kien X. Nguyen, Arthur Trembanis, and Xi Peng</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of&nbsp;the Annual Conference on Neural Information Processing Systems&nbsp; (NeurIPS) Datasets and Benchmarks Track, 2024.</p>
                                    <div id="neurips24_kien_group" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank" href="https://arxiv.org/pdf/2411.00172">Paper</a><a class="btn btn-outline-primary" role="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank" href="https://sites.google.com/udel.edu/seafloorai/home">Dataset</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-target="#neurips24_kien_abs" aria-expanded="false" aria-controls="eccv24_li_abs-1" data-bs-toggle="collapse">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#neurips24_kien" aria-expanded="false" aria-controls="eccv24_li-1">BibTeX</button>
                                        <div id="neurips24_kien" class="collapse" data-bs-parent="#neurips24_kien_group">
                                            <div class="card">
                                                <div class="card-body" style="background: var(--bs-light);">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{nguyenseafloorai,<br>&nbsp;title={SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey},<br>&nbsp;author={Nguyen, Kien X and Qiao, Fengchun and Trembanis, Arthur and Peng, Xi},<br>&nbsp;booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and&nbsp; Benchmarks Track},<br>&nbsp;year={2024}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="neurips24_kien_abs" class="collapse" data-bs-parent="#neurips24_kien_group">
                                            <div class="card">
                                                <div class="card-body"><p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">A major obstacle to the advancements of machine learning models in marine science, particularly in sonar imagery analysis, is the scarcity of AI-ready datasets. While there have been efforts to make AI-ready sonar image dataset publicly available, they suffer from limitations in terms of environment setting and scale. To bridge this gap, we introduce <tt>SeafloorAI</tt> and <tt>SeafloorGenAI</tt>, the first extensive AI-ready datasets for seafloor mapping across 5 geological layers. These datasets, curated in collaboration with marine scientists, facilitate both <em>vision</em> and <em>vision-language</em>-capable machine learning models for sonar imagery. The dataset consists of 62 geo-distributed data surveys across 17,300 square kilometers, with 696K sonar images, 827K annotated segmentation masks, and approximately 7M question-answer pairs. By making our data processing source code publicly available, we aim to engage the marine science community to enrich the data pool and inspire the machine learning community to develop more robust models. This collaborative approach will enhance the capabilities and applications of our datasets within both fields.</p></div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">NeurIPS</p><img src="assets/img/conference/Kien_NeurIPS24.png">
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>DEAL: Disentangle and Localize Concept-level Explanations for VLMs</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ECCV'24 Strong Double Blind<strong>]</strong></span>&nbsp;Tang Li, Mengmeng Ma, and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of the European Conference on Computer Vision (ECCV), 2024.</p>
                                    <div id="eccv24_li_group-2" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05615.pdf" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://www.youtube.com/watch?v=Rd4vFWI2fKw" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Video</a><a class="btn btn-outline-primary" role="button" href="https://github.com/tangli-udel/DEAL" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#eccv24_li_abs-2" aria-expanded="false" aria-controls="eccv24_li_abs-1">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#eccv24_li-2" aria-expanded="false" aria-controls="eccv24_li-1">BibTeX</button>
                                        <div id="eccv24_li-2" class="collapse" data-bs-parent="#eccv24_li_group-2">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{li2024deal,<br> title={DEAL: Disentangle and Localize Concept-level Explanations for VLMs},<br> author={Li, Tang and Ma, Mengmeng and Peng, Xi},<br> booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},<br> year={2024}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="eccv24_li_abs-2" class="collapse" data-bs-parent="#eccv24_li_group-1">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">Large pre-trained Vision-Language Models (VLMs) have become ubiquitous foundational components of other models and downstream tasks. Although powerful, our empirical results reveal that such models might not be able to identify fine-grained concepts. Specifically, the explanations of VLMs with respect to fine-grained concepts are entangled and mislocalized. To address this issue, we propose to DisEntAngle and Localize (DEAL) the concept-level explanations for VLMs without human annotations. The key idea is encouraging the concept-level explanations to be distinct while maintaining consistency with category-level explanations. We conduct extensive experiments and ablation studies on a wide range of benchmark datasets and vision-language models. Our empirical results demonstrate that the proposed method significantly improves the concept-level explanations of the model in terms of disentanglability and localizability. Surprisingly, the improved explainability alleviates the model's reliance on spurious correlations, which further benefits the prediction accuracy.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">ECCV</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/Rd4vFWI2fKw?si=c73JeLW_XHGjgBRQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Are Data-driven Explanations Robust against Out-of-distribution Data?</strong><br></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVPR'23<strong>]</strong></span>&nbsp;Tang Li, Fengchun Qiao, Mengmeng Ma, and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023.&nbsp;</p>
                                    <div id="cvpr23_group-1" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Are_Data-Driven_Explanations_Robust_Against_Out-of-Distribution_Data_CVPR_2023_paper.pdf" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://www.youtube.com/watch?v=4-8zMdB83x8" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#cvpr23abs-1" aria-expanded="false" aria-controls="cvpr23abs-1">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#cvpr23-1" aria-expanded="false" aria-controls="cvpr23-1">BibTeX</button>
                                        <div id="cvpr23-1" class="collapse" data-bs-parent="#cvpr23_group-1">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{tang2023dre,<br>&nbsp;title={Are Data-driven Explanations Robust against Out-of-distribution Data?},<br>&nbsp;author={Li, Tang and Qiao, Fengchun and Ma, Mengmeng and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>&nbsp;year={2023}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="cvpr23abs-1" class="collapse" data-bs-parent="#cvpr23_group-1">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="margin-bottom: 0px;font-size: 12px;text-align: justify;">As black-box models increasingly power high-stakes applications, a variety of data-driven explanation methods have been introduced. Meanwhile, machine learning models are constantly challenged by distributional shifts. A question naturally arises: Are data-driven explanations robust against out-of-distribution data? Our empirical results show that even though made accurate predictions, the model might still yield unreliable explanations under distribution shifts. How to develop robust explanations against out-of-distribution data? To address this problem, we propose an end-to-end model-agnostic learning framework Distributionally Robust Explanations (DRE). The key idea is, inspired by self-supervised learning, to fully utilizes the inter-distribution information to provide supervisory signals for the learning of explanations without human annotation. Can robust explanations benefit the model’s generalization capability? We conduct extensive experiments on a wide range of tasks and data types, including classification and regression on image and scientific tabular data. Our results demonstrate that the proposed method significantly improves the model performance in terms of explanation and prediction robustness against distributional shifts.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">CVPR</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/4-8zMdB83x8?si=vNvGrIZoOhbn9tVO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 0px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Deep Learning for Spatiotemporal Modeling of Urbanization</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[NeurIPS'21W Best Paper Award]</span>&nbsp;Tang Li, Jing Gao, and Xi Peng.&nbsp;</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);"><span style="font-weight: bold;font-style: italic;color: var(--bs-gray-900);"></span>In Proceedings of NeurIPS 2021 Machine Learning in Public Health Workshop. </p>
                                    <div id="neurips21w_group-1" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://arxiv.org/pdf/2112.09668.pdf" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://www.youtube.com/watch?v=3VyVck_gv4g" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#neurips21wabs-1" aria-expanded="false" aria-controls="neurips21wabs-1">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#neurips21w-1" aria-expanded="false" aria-controls="neurips21w-1">BibTeX</button><a class="btn btn-outline-primary" role="button" href="https://sites.google.com/nyu.edu/mlph2021/accepted-papers?authuser=0#:~:text=Deep%20Learning%20for%20Spatiotemporal,Peng%20(University%20of%20Delaware)" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Award</a>
                                        <div id="neurips21w-1" class="collapse" data-bs-parent="#neurips21w_group-1">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@article{li2021deep,<br>title={Deep Learning for Spatiotemporal Modeling of Urbanization},<br>author={Tang Li and Gao, Jing and Xi Peng},<br>journal={Advances in Neural Information Processing Systems Workshops (Best Paper Award)},<br>year={2021}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="neurips21wabs-1" class="collapse" data-bs-parent="#neurips21w_group-1">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">Urbanization has a strong impact on the health and wellbeing of populations across the world. Predictive spatial modeling of urbanization therefore can be a useful tool for effective public health planning. Many spatial urbanization models have been developed using classic machine learning and numerical modeling techniques. However, deep learning with its proven capacity to capture complex spatiotemporal phenomena has not been applied to urbanization modeling. Here we explore the capacity of deep spatial learning for the predictive modeling of urbanization. We treat numerical geospatial data as images with pixels and channels, and enrich the dataset by augmentation, in order to leverage the high capacity of deep learning. Our resulting model can generate end-to-end multi-variable urbanization predictions, and outperforms a state-of-the-art classic machine learning urbanization model in preliminary comparisons.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">NeurIPS</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/3VyVck_gv4g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <div id="scalable_learning" class="block-content" style="padding-right: 1%;padding-left: 1%;">
                    <div class="col">
                        <h1 style="font-size: calc(15pt + 0.7vw);text-align: center;margin-bottom: 20px;">Multimodal Learning with Missing Modality</h1>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition</strong></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICCV'23]</span>&nbsp;Qitong Wang, Long Zhao, Liangzhe Yuan, Ting Liu, and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of IEEE International Conference on Computer Vision, 2023.</p>
                                    <div id="aaai21_group-1" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Learning_from_Semantic_Alignment_between_Unpaired_Multiviews_for_Egocentric_Video_ICCV_2023_paper.pdf" style="margin-right: 25px;margin-left: 0px;width: 95px;" target="_blank">Paper</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#iccv23abs-2" aria-expanded="false" aria-controls="aaai21abs-2">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#iccv23-2" aria-expanded="false" aria-controls="aaai21-2">BibTeX</button>
                                        <div id="iccv23-2" class="collapse" data-bs-parent="#aaai21_group-2">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{Wang2023LearningFS,<br> title={Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition},<br> author={Qitong Wang and Long Zhao and Liangzhe Yuan and Ting Liu and Xi Peng},<br> booktitle={Proceedings of the International Conference on Computer Vision (ICCV)},<br> year={2023},<br> url={https://api.semanticscholar.org/CorpusID:261064926}}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="iccv23abs-2" class="collapse" data-bs-parent="#aaai21_group-2">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">We are concerned with a challenging scenario in unpaired multiview video learning. In this case, the model aims to learn comprehensive multiview representations while the cross-view semantic information exhibits variations. We propose Semantics-based Unpaired Multiview Learning (SUM-L) to tackle this unpaired multiview learning problem. The key idea is to build cross-view pseudopairs and do view-invariant alignment by leveraging the semantic information of videos. To facilitate the data efficiency of multiview learning, we further perform video-text alignment for first-person and third-person videos, to fully leverage the semantic knowledge to improve video representations. Extensive experiments on multiple benchmark datasets verify the effectiveness of our framework. Our method also outperforms multiple existing view-alignment methods, under the more challenging scenario than typical paired or unpaired multimodal or multiview learning. Our code is available at https://github.com/wqtwjt1996/SUM-L.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">ICCV</p><img src="assets/img/conference/ICCV23_Qitong.png">
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>Are Multimodal Transformers Robust to Missing Modality?</strong></h3>
                                    <div class="info" style="font-size: calc(8pt + 0.5vw);"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[CVPR'22<strong>]</strong></span>&nbsp;Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, and Xi Peng.</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022.&nbsp;</p>
                                    <div id="cvpr22meng_group-2" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://drive.google.com/file/d/1yZMkuZXEfk_OvPoVI6Ut1e_4MyTm1oLL/view?usp=sharing" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#cvpr22mengabs-2" aria-expanded="false" aria-controls="cvpr22mengabs-2">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#cvpr22meng-2" aria-expanded="false" aria-controls="cvpr22meng-2">BibTeX</button>
                                        <div id="cvpr22meng-2" class="collapse" data-bs-parent="#cvpr22meng_group-2">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{ma2022multimodal,<br>&nbsp;title={Are Multimodal Transformers Robust to Missing Modality?},<br>&nbsp;author={Ma, Mengmeng and Ren, Jian and Zhao, Long and Testuggine, Davide and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br>&nbsp;pages={18177--18186},<br>&nbsp;year={2022}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="cvpr22mengabs-2" class="collapse" data-bs-parent="#cvpr22meng_group-2">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">Multimodal data collected from the real world are often imperfect due to missing modalities. Therefore multimodal models that are robust against modal-incomplete data are highly preferred. Recently, Transformer models have shown great success in processing multimodal data. However, existing work has been limited to either architecture designs or pre-training strategies; whether Transformer models are naturally robust against missing-modal data has rarely been investigated. In this paper, we present the firstof-its-kind work to comprehensively investigate the behavior of Transformers in the presence of modal-incomplete data. Unsurprising, we find Transformer models are sensitive to missing modalities while different modal fusion strategies will significantly affect the robustness. What surprised us is that the optimal fusion strategy is dataset dependent even for the same Transformer model; there does not exist a universal strategy that works in general cases. Based on these findings, we propose a principle method to improve the robustness of Transformer models by automatically searching for an optimal fusion strategy regarding input data. Experimental validations on three benchmarks support the superior performance of the proposed method.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">CVPR</p><div class="iframe-container"><iframe width="352" height="198" src="https://drive.google.com/file/d/1yZMkuZXEfk_OvPoVI6Ut1e_4MyTm1oLL/preview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>SMIL: Multimodal Learning with Severely Missing Modality</strong><br></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[AAAI'21]</span>&nbsp;Mengmeng Ma, Jian Ren, Long Zhao,&nbsp;Sergey Tulyakov, Cathy Wu, Xi Peng.&nbsp;</span></div>
                                    <p style="font-size: calc(12px + 0.2vw);color: var(--bs-gray-600);">In&nbsp;Proceedings of the Association for the Advancement of Artificial Intelligence, 2021.&nbsp;</p>
                                    <div id="aaai21_group-2" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://cdn.aaai.org/ojs/16330/16330-13-19824-1-2-20210518.pdf" style="margin-right: 25px;margin-left: 0px;width: 95px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://www.youtube.com/watch?v=1_yRw_W8KTc" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Video</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#aaai21abs-2" aria-expanded="false" aria-controls="aaai21abs-2">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#aaai21-2" aria-expanded="false" aria-controls="aaai21-2">BibTeX</button>
                                        <div id="aaai21-2" class="collapse" data-bs-parent="#aaai21_group-2">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{ma2021smil,<br>&nbsp;title={SMIL: Multimodal learning with severely missing modality},<br>&nbsp;author={Ma, Mengmeng and Ren, Jian and Zhao, Long and Tulyakov, Sergey and Wu, Cathy and Peng, Xi},<br>&nbsp;booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>&nbsp;volume={35},<br>&nbsp;number={3},<br>&nbsp;pages={2302--2310},<br>&nbsp;year={2021}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="aaai21abs-2" class="collapse" data-bs-parent="#aaai21_group-2">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">A common assumption in multimodal learning is the completeness of training data, i.e. full modalities are available in all training examples. Although there exists research endeavor in developing novel methods to tackle the incompleteness of testing data, e.g., modalities are partially missing in testing examples, few of them can handle incomplete training modalities. The problem becomes even more challenging if considering the case of severely missing, e.g., 90% training examples may have incomplete modalities. For the first time in the literature, this paper formally studies multimodal learning with missing modality in terms of flexibility (missing modalities in training, testing, or both) and efficiency (most training data have incomplete modality). Technically, we propose a new method named SMIL that leverages Bayesian meta-learning in uniformly achieving both objectives. To validate our idea, we conduct a series of experiments on three popular benchmarks: MM-IMDb, CMUMOSI, and avMNIST. The results prove the state-of-the-art performance of SMIL over existing methods and generative baselines including autoencoders and generative adversarial networks.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">AAAI</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/75P535ob5-Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 70px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>A Good Image Generator Is What You Need for High-Resolution Video Synthesis</strong><br></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICLR'21 Spotlight]</span>&nbsp;Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, Sergey Tulyakov.</span></div>
                                    <div id="iclr21_group-1" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openreview.net/forum?id=6puCSjH3hwA" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a><a class="btn btn-outline-primary" role="button" href="https://papertalk.org/papertalks/29015" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Video</a><a class="btn btn-outline-primary" role="button" href="https://github.com/snap-research/MoCoGAN-HD" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Code</a><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#iclr21abs-1" aria-expanded="false" aria-controls="iclr21abs-1">Abstract</button><button class="btn btn-outline-primary" type="button" style="margin-right: 25px;width: 95px;margin-left: 0px;" data-bs-toggle="collapse" data-bs-target="#iclr21-1" aria-expanded="false" aria-controls="iclr21-1">BibTeX</button>
                                        <div id="iclr21-1" class="collapse" data-bs-parent="#iclr21_group-1">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{tian2020good,<br>&nbsp;title={A Good Image Generator Is What You Need for High-Resolution Video Synthesis},<br>&nbsp;author={Tian, Yu and Ren, Jian and Chai, Menglei and Olszewski, Kyle and Peng, Xi and Metaxas, Dimitris N and Tulyakov, Sergey},<br>&nbsp;booktitle={International Conference on Learning Representations},<br>&nbsp;year={2020}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="iclr21abs-1" class="collapse" data-bs-parent="#iclr21_group-1">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">ICLR</p><div class="iframe-container"><iframe width="352" height="198" src="https://www.youtube.com/embed/bucl2kXMbSM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div>
                            </div>
                        </div>
                        <div class="clean-blog-post" style="padding-bottom: 0px;">
                            <div class="row" style="margin: 0px;">
                                <div class="col-lg-7 col-xxl-8" style="margin-bottom: 10px;">
                                    <h3 style="font-family: Adamina, serif;font-size: calc(18px + 0.2vw);"><strong>AdaTransform: Adaptive Data Transformation</strong><br></h3>
                                    <div class="info"><span style="font-size: calc(14px + 0.2vw);font-family: 'Times New Roman Cyr';"><span style="font-weight: bold;font-family: Montserrat, sans-serif;">[ICCV'19 Oral]</span>&nbsp;Zhiqiang Tang, Xi Peng , Tingfeng Li, Yizhe Zhu, Dimitris N Metaxas.</span></div>
                                    <div id="iclr21_group-2" class="container" style="padding-left: 0px;"><a class="btn btn-outline-primary" role="button" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_AdaTransform_Adaptive_Data_Transformation_ICCV_2019_paper.pdf" style="margin-right: 25px;width: 95px;margin-left: 0px;" target="_blank">Paper</a>
                                        <div id="iclr-2" class="collapse" data-bs-parent="#iclr21_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;">@inproceedings{tian2020good,<br>&nbsp;title={A Good Image Generator Is What You Need for High-Resolution Video Synthesis},<br>&nbsp;author={Tian, Yu and Ren, Jian and Chai, Menglei and Olszewski, Kyle and Peng, Xi and Metaxas, Dimitris N and Tulyakov, Sergey},<br>&nbsp;booktitle={International Conference on Learning Representations},<br>&nbsp;year={2020}<br>}</p>
                                                </div>
                                            </div>
                                        </div>
                                        <div id="iclr21abs-2" class="collapse" data-bs-parent="#iclr21_group">
                                            <div class="card">
                                                <div class="card-body">
                                                    <p class="text-secondary" style="font-size: 12px;font-family: Montserrat, sans-serif;margin-bottom: 0px;text-align: justify;">Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques.</p>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="col-lg-5 col-xxl-4">
                                    <p style="text-align: center;font-weight: bold;background: rgb(0,63,157);color: var(--bs-white);z-index: 9999;width: calc(80px + 1.5vw);margin-bottom: 0px;padding-top: 0px;margin-top: 10px;position: absolute;margin-left: -25px;font-size: calc(14px + 0.5vw);">ICCV</p><img src="assets/img/conference/AdaTransform.jpeg">
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="clean-block clean-gallery dark" style="background: var(--bs-white);padding-bottom: 50px;">
            <div class="container">
                <div class="block-heading" style="padding-top: 0px;">
                    <h2 class="text-info" style="padding-top: 30px;">Sponsors</h2>
                </div>
                <div class="row">
                    <div class="col-md-6 col-lg-4 col-xxl-2 item" style="width: 100%;"><img src="assets/img/sponsor/NSF_logo.png" style="width: 8.2%;margin: 0.4%;padding: 0.5%;"><img src="assets/img/sponsor/NIH.png" style="width: 8.2%;margin: 0.4%;padding: 0.5%;"><img src="assets/img/sponsor/dod.png" style="width: 8.2%;margin: 0.4%;padding: 0.5%;"><img src="assets/img/sponsor/cdc.jpeg" style="width: 8.2%;margin: 0.4%;"><img src="assets/img/sponsor/MSKCC.png" style="width: 8.2%;margin: 0.4%;"><img src="assets/img/sponsor/Google-research.jpeg" style="width: 8.2%;margin: 0.4%;"><img src="assets/img/sponsor/snapchat.png" style="width: 8.2%;margin: 0.4%;padding: 1%;"><img src="assets/img/sponsor/gur.png" style="width: 8.2%;margin: 0.4%;"><img src="assets/img/sponsor/udrf.png" style="width: 8.2%;margin: 0.4%;"><img src="assets/img/sponsor/udrf-si.png" style="width: 8.2%;margin: 0.4%;"><img src="assets/img/sponsor/aicoe.png" style="width: 8.2%;margin: 0.4%;"></div>
                </div>
            </div>
        </section>
    </main><button class="btn btn-light" data-bss-hover-animate="pulse" id="myBtn" title="Go To Top" onclick="topFunction()" type="button" style="background: rgba(164,164,164,0.42);"><i class="fa fa-chevron-up" style="margin-left: 5px;"></i></button>
    <footer class="page-footer dark">
        <div class="footer-copyright">
            <p>DeepREAL © 2024&nbsp;<span style="color: rgb(209, 210, 211); background-color: rgb(34, 37, 41);">All Rights Reserved</span>&nbsp;&nbsp;|&nbsp; designed by <span style="font-style: italic;">Tang Li</span></p>
        </div>
    </footer>
    <script src="assets/bootstrap/js/bootstrap.min.js"></script>
    <script src="assets/js/bs-init.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/baguettebox.js/1.10.0/baguetteBox.min.js"></script>
    <script src="assets/js/Back-To-Top.js"></script>
    <script src="assets/js/vanilla-zoom.js"></script>
    <script src="assets/js/back-to-top-button.js"></script>
    <script src="assets/js/back-to-top-scroll.js"></script>
    <script src="assets/js/theme.js"></script>
    <script src="assets/js/Bold-BS4-Animated-Back-To-Top.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Swiper/6.4.8/swiper-bundle.min.js"></script>
    <script src="assets/js/MUSA_back-to-top.js"></script>
    <script src="assets/js/Popup-Element-Overlay.js"></script>
    <script src="assets/js/responsive-blog-card-slider-1.js"></script>
    <script src="assets/js/responsive-blog-card-slider.js"></script>
    <script src="assets/js/Simple-Slider.js"></script>
    <script src="assets/js/Swiper-Slider-Card-For-Blog-Or-Product.js"></script>
    <script src="assets/js/Tabbed_slider.js"></script>
    <script src="assets/js/WOWSlider-about-us.js"></script>
</body>

</html>